{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_uFXbSdHquIT"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "dataset_names = sns.get_dataset_names()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "QC4DjSMPrRTD",
        "outputId": "cacfd85d-b17f-4f44-97ed-0d4fb839e496"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d4dc9c1e-2035-4440-9b15-0ccc73660ab0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "      <th>class</th>\n",
              "      <th>who</th>\n",
              "      <th>adult_male</th>\n",
              "      <th>deck</th>\n",
              "      <th>embark_town</th>\n",
              "      <th>alive</th>\n",
              "      <th>alone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>no</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "      <td>First</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>C</td>\n",
              "      <td>Cherbourg</td>\n",
              "      <td>yes</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>yes</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "      <td>First</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>C</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>yes</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>no</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>S</td>\n",
              "      <td>Second</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>no</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>S</td>\n",
              "      <td>First</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>B</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>yes</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23.4500</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>no</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>C</td>\n",
              "      <td>First</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>C</td>\n",
              "      <td>Cherbourg</td>\n",
              "      <td>yes</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>Q</td>\n",
              "      <td>Third</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Queenstown</td>\n",
              "      <td>no</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows × 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4dc9c1e-2035-4440-9b15-0ccc73660ab0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d4dc9c1e-2035-4440-9b15-0ccc73660ab0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d4dc9c1e-2035-4440-9b15-0ccc73660ab0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     survived  pclass     sex   age  ...  deck  embark_town  alive  alone\n",
              "0           0       3    male  22.0  ...   NaN  Southampton     no  False\n",
              "1           1       1  female  38.0  ...     C    Cherbourg    yes  False\n",
              "2           1       3  female  26.0  ...   NaN  Southampton    yes   True\n",
              "3           1       1  female  35.0  ...     C  Southampton    yes  False\n",
              "4           0       3    male  35.0  ...   NaN  Southampton     no   True\n",
              "..        ...     ...     ...   ...  ...   ...          ...    ...    ...\n",
              "886         0       2    male  27.0  ...   NaN  Southampton     no   True\n",
              "887         1       1  female  19.0  ...     B  Southampton    yes   True\n",
              "888         0       3  female   NaN  ...   NaN  Southampton     no  False\n",
              "889         1       1    male  26.0  ...     C    Cherbourg    yes   True\n",
              "890         0       3    male  32.0  ...   NaN   Queenstown     no   True\n",
              "\n",
              "[891 rows x 15 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = sns.load_dataset('titanic')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh6vvJoAw4Xn"
      },
      "source": [
        "### 1. 데이터 전처리\n",
        "- Faeture Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "CvggE8lBw4VV",
        "outputId": "b51b6647-2a69-4435-ec35-25a6bf3281a6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fa22609a-680e-45d4-b317-9f74b230a404\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "      <th>deck</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa22609a-680e-45d4-b317-9f74b230a404')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fa22609a-680e-45d4-b317-9f74b230a404 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fa22609a-680e-45d4-b317-9f74b230a404');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   survived  pclass     sex   age  sibsp  parch     fare embarked deck\n",
              "0         0       3    male  22.0      1      0   7.2500        S  NaN\n",
              "1         1       1  female  38.0      1      0  71.2833        C    C\n",
              "2         1       3  female  26.0      0      0   7.9250        S  NaN"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[['survived','pclass','sex', 'age','sibsp','parch','fare','embarked','deck']]\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUjhYI3uw4TM"
      },
      "source": [
        "- 결측치 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9FmUmdVvw4Rb",
        "outputId": "11dda13c-f52b-4520-8848-712116c0f191"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "survived      0\n",
              "pclass        0\n",
              "sex           0\n",
              "age         177\n",
              "sibsp         0\n",
              "parch         0\n",
              "fare          0\n",
              "embarked      2\n",
              "deck        688\n",
              "dtype: int64"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XIWdgnQUw4PE",
        "outputId": "004ba59f-3995-494d-aaf8-682c15884161"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/series.py:4536: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  downcast=downcast,\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# age 컬럼은 평균나이로 대체\n",
        "df.age.fillna(df.age.mean(),inplace=True)\n",
        "df.age.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PJ_foPU0x8Iu",
        "outputId": "35418e3a-bbbf-46dd-f970-bc5676da360e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "S    644\n",
              "C    168\n",
              "Q     77\n",
              "Name: embarked, dtype: int64"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# embarked 칼럼은 최빈값으로 대체\n",
        "df.embarked.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qwFmJNqWx8G8",
        "outputId": "ffdedcf6-a605-4b2a-c242-c6618e3b8bda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/series.py:4536: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  downcast=downcast,\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 최빈값은 'S' 이므로 'S' 값으로 대체하기\n",
        "df.embarked.fillna('S', inplace=True)\n",
        "df.embarked.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "12SGdyVcx8Ee",
        "outputId": "5e248a6a-35a0-4372-c908-634af5837eb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ],
      "source": [
        "# Deck 칼럼은 결측치가 많아서 삭제해준다.\n",
        "df.drop(columns = ['deck'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "KcXbz7Fix8CU",
        "outputId": "5dd511fd-b3ec-4492-e414-395e39c5aad9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1efdbba9-7a81-4268-b588-acaa7798a631\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1efdbba9-7a81-4268-b588-acaa7798a631')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1efdbba9-7a81-4268-b588-acaa7798a631 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1efdbba9-7a81-4268-b588-acaa7798a631');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   survived  pclass     sex   age  sibsp  parch     fare embarked\n",
              "0         0       3    male  22.0      1      0   7.2500        S\n",
              "1         1       1  female  38.0      1      0  71.2833        C\n",
              "2         1       3  female  26.0      0      0   7.9250        S"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeHUCpH2x8AM"
      },
      "source": [
        "#### - 카테고리 값인 \"sex\" 와 \"embarked\" 컬럼은 숫자로 변환\n",
        "- LabelEncoder로 변환하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_5z1vcnOx798"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "jmXxFMUhx773",
        "outputId": "a7474f0c-32b7-4013-cec5-e6cac06a27fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e6ab1ec9-8fb4-473b-a664-4d797381ac9a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6ab1ec9-8fb4-473b-a664-4d797381ac9a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e6ab1ec9-8fb4-473b-a664-4d797381ac9a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e6ab1ec9-8fb4-473b-a664-4d797381ac9a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   survived  pclass  sex   age  sibsp  parch     fare  embarked\n",
              "0         0       3    1  22.0      1      0   7.2500         2\n",
              "1         1       1    0  38.0      1      0  71.2833         0\n",
              "2         1       3    0  26.0      0      0   7.9250         2"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sex = le.fit_transform(df.sex)\n",
        "df.embarked = le.fit_transform(df.embarked)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KzRe8XJK4zRX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "seed = 2022\n",
        "np.random.seed(2022)\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PVYqkQQ4wPI"
      },
      "source": [
        "- Train/Test dataset으로 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ayvw1uU9x750",
        "outputId": "53a9413e-c4ea-47de-c559-edbdb6125b34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((891, 7), (891,))"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = df.iloc[:,1:].values\n",
        "y = df.survived.values\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Bjd4T96ax73w",
        "outputId": "4719ea10-a935-4dbc-b7a4-b67709ebee5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((712, 7), (179, 7), (712,), (179,))"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.2, random_state=seed\n",
        ")\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjHByjnPx71c"
      },
      "source": [
        "- 모델 정의/설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "9tp9MMOIx7zU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zP03JHnOx7xP",
        "outputId": "b220b489-a640-474a-c088-7fabfaff18e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 30)                240       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 12)                372       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 8)                 104       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 725\n",
            "Trainable params: 725\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "t_model = Sequential([\n",
        "    Dense(30, input_dim = 7, activation='relu'),\n",
        "    Dense(12, activation='relu'),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid'),\n",
        "])\n",
        "t_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "i5b8LnQhx7u9"
      },
      "outputs": [],
      "source": [
        "t_model.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = \"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvl2rJVQx7tF"
      },
      "source": [
        "- 모델 저장 관련 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "PDZqdQ0tx7q-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"t_model\"):\n",
        "    os.mkdir(\"t_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "vR8YZsNLx7o4"
      },
      "outputs": [],
      "source": [
        "t_model_path = \"t_model/titanic_{epoch:03d}_{val_loss:.4f}.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "poUm5iXQx7m0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "ck = ModelCheckpoint(\n",
        "    t_model_path, monitor='val_loss', verbose=1, save_best_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPpmH0s2x7ks"
      },
      "source": [
        "- 모델 학습 및 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pv8UkNVSx7il",
        "outputId": "d2381fdb-1278-4331-dcb4-7fe5bd0ee642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.76988, saving model to t_model/titanic_001_0.7699.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.76988\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.76988\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.76988 to 0.74608, saving model to t_model/titanic_004_0.7461.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.74608 to 0.74133, saving model to t_model/titanic_005_0.7413.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.74133\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.74133\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.74133 to 0.72054, saving model to t_model/titanic_008_0.7205.h5\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.72054 to 0.70792, saving model to t_model/titanic_009_0.7079.h5\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.70792 to 0.70097, saving model to t_model/titanic_010_0.7010.h5\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.70097 to 0.69295, saving model to t_model/titanic_011_0.6930.h5\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.69295 to 0.68493, saving model to t_model/titanic_012_0.6849.h5\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.68493 to 0.68037, saving model to t_model/titanic_013_0.6804.h5\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.68037 to 0.67492, saving model to t_model/titanic_014_0.6749.h5\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.67492 to 0.66610, saving model to t_model/titanic_015_0.6661.h5\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.66610 to 0.66079, saving model to t_model/titanic_016_0.6608.h5\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.66079 to 0.65464, saving model to t_model/titanic_017_0.6546.h5\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.65464 to 0.64937, saving model to t_model/titanic_018_0.6494.h5\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.64937 to 0.64439, saving model to t_model/titanic_019_0.6444.h5\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.64439 to 0.64032, saving model to t_model/titanic_020_0.6403.h5\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.64032 to 0.63700, saving model to t_model/titanic_021_0.6370.h5\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.63700 to 0.63222, saving model to t_model/titanic_022_0.6322.h5\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.63222 to 0.62960, saving model to t_model/titanic_023_0.6296.h5\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.62960 to 0.62599, saving model to t_model/titanic_024_0.6260.h5\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.62599 to 0.62481, saving model to t_model/titanic_025_0.6248.h5\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.62481 to 0.61955, saving model to t_model/titanic_026_0.6196.h5\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.61955 to 0.61679, saving model to t_model/titanic_027_0.6168.h5\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.61679 to 0.61481, saving model to t_model/titanic_028_0.6148.h5\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.61481 to 0.61419, saving model to t_model/titanic_029_0.6142.h5\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.61419\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.61419 to 0.61352, saving model to t_model/titanic_031_0.6135.h5\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.61352 to 0.61215, saving model to t_model/titanic_032_0.6121.h5\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.61215 to 0.61046, saving model to t_model/titanic_033_0.6105.h5\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.61046 to 0.60831, saving model to t_model/titanic_034_0.6083.h5\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.60831 to 0.60527, saving model to t_model/titanic_035_0.6053.h5\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.60527 to 0.60369, saving model to t_model/titanic_036_0.6037.h5\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.60369 to 0.60337, saving model to t_model/titanic_037_0.6034.h5\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.60337 to 0.60190, saving model to t_model/titanic_038_0.6019.h5\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.60190 to 0.60111, saving model to t_model/titanic_039_0.6011.h5\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.60111 to 0.59947, saving model to t_model/titanic_040_0.5995.h5\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.59947 to 0.59749, saving model to t_model/titanic_041_0.5975.h5\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.59749 to 0.59612, saving model to t_model/titanic_042_0.5961.h5\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.59612 to 0.59512, saving model to t_model/titanic_043_0.5951.h5\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.59512\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.59512\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.59512\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.59512\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.59512 to 0.59196, saving model to t_model/titanic_048_0.5920.h5\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.59196 to 0.59126, saving model to t_model/titanic_049_0.5913.h5\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.59126 to 0.59000, saving model to t_model/titanic_050_0.5900.h5\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.59000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.59000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.59000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.59000\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.59000 to 0.58897, saving model to t_model/titanic_055_0.5890.h5\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.58897 to 0.58713, saving model to t_model/titanic_056_0.5871.h5\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.58713 to 0.58677, saving model to t_model/titanic_057_0.5868.h5\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.58677\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.58677 to 0.58561, saving model to t_model/titanic_059_0.5856.h5\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.58561 to 0.58331, saving model to t_model/titanic_060_0.5833.h5\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.58331 to 0.58328, saving model to t_model/titanic_061_0.5833.h5\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.58328\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.58328 to 0.58296, saving model to t_model/titanic_063_0.5830.h5\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.58296 to 0.58159, saving model to t_model/titanic_064_0.5816.h5\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.58159 to 0.58139, saving model to t_model/titanic_065_0.5814.h5\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.58139 to 0.57997, saving model to t_model/titanic_066_0.5800.h5\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.57997 to 0.57981, saving model to t_model/titanic_067_0.5798.h5\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.57981 to 0.57828, saving model to t_model/titanic_068_0.5783.h5\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.57828 to 0.57743, saving model to t_model/titanic_069_0.5774.h5\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.57743 to 0.57558, saving model to t_model/titanic_070_0.5756.h5\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.57558\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.57558\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.57558 to 0.57393, saving model to t_model/titanic_073_0.5739.h5\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.57393 to 0.57179, saving model to t_model/titanic_074_0.5718.h5\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.57179 to 0.57113, saving model to t_model/titanic_075_0.5711.h5\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.57113\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.57113\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.57113\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.57113 to 0.56864, saving model to t_model/titanic_079_0.5686.h5\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.56864 to 0.56730, saving model to t_model/titanic_080_0.5673.h5\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.56730\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.56730\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.56730\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.56730\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.56730 to 0.56620, saving model to t_model/titanic_085_0.5662.h5\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.56620\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.56620\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.56620 to 0.56393, saving model to t_model/titanic_088_0.5639.h5\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.56393 to 0.56245, saving model to t_model/titanic_089_0.5625.h5\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.56245 to 0.55900, saving model to t_model/titanic_090_0.5590.h5\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.55900\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.55900\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.55900\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.55900\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.55900 to 0.55674, saving model to t_model/titanic_095_0.5567.h5\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.55674\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.55674\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.55674 to 0.55494, saving model to t_model/titanic_098_0.5549.h5\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.55494\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.55494\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.55494 to 0.55416, saving model to t_model/titanic_101_0.5542.h5\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.55416\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.55416\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.55416\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.55416 to 0.55203, saving model to t_model/titanic_105_0.5520.h5\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.55203 to 0.54895, saving model to t_model/titanic_106_0.5490.h5\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.54895\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.54895\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.54895\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.54895\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.54895\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.54895\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.54895\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.54895 to 0.54843, saving model to t_model/titanic_114_0.5484.h5\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.54843 to 0.54467, saving model to t_model/titanic_115_0.5447.h5\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.54467\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.54467\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.54467\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.54467 to 0.54423, saving model to t_model/titanic_119_0.5442.h5\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.54423 to 0.54099, saving model to t_model/titanic_120_0.5410.h5\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.54099\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.54099\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.54099\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.54099\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.54099 to 0.54017, saving model to t_model/titanic_125_0.5402.h5\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.54017\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.54017 to 0.53852, saving model to t_model/titanic_127_0.5385.h5\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.53852 to 0.53604, saving model to t_model/titanic_128_0.5360.h5\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.53604\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.53604\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.53604\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.53604\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.53604 to 0.53553, saving model to t_model/titanic_133_0.5355.h5\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.53553\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.53553\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.53553 to 0.53433, saving model to t_model/titanic_136_0.5343.h5\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.53433 to 0.53431, saving model to t_model/titanic_137_0.5343.h5\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.53431\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.53431 to 0.53136, saving model to t_model/titanic_139_0.5314.h5\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.53136\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.53136\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.53136\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.53136\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.53136\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.53136\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.53136 to 0.53105, saving model to t_model/titanic_146_0.5311.h5\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.53105 to 0.52966, saving model to t_model/titanic_147_0.5297.h5\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.52966\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.52966\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.52966\n",
            "\n",
            "Epoch 00151: val_loss improved from 0.52966 to 0.52865, saving model to t_model/titanic_151_0.5287.h5\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.52865\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.52865 to 0.52808, saving model to t_model/titanic_153_0.5281.h5\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.52808\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.52808\n",
            "\n",
            "Epoch 00156: val_loss improved from 0.52808 to 0.52645, saving model to t_model/titanic_156_0.5264.h5\n",
            "\n",
            "Epoch 00157: val_loss improved from 0.52645 to 0.52357, saving model to t_model/titanic_157_0.5236.h5\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.52357\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.52357\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.52357\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.52357\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.52357\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.52357\n",
            "\n",
            "Epoch 00164: val_loss improved from 0.52357 to 0.52344, saving model to t_model/titanic_164_0.5234.h5\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.52344\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.52344\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.52344\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.52344\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.52344\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.52344 to 0.52225, saving model to t_model/titanic_170_0.5222.h5\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.52225\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.52225\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.52225\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.52225\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.52225\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.52225 to 0.52168, saving model to t_model/titanic_176_0.5217.h5\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.52168\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.52168\n"
          ]
        }
      ],
      "source": [
        "hist = t_model.fit(X_train, y_train, validation_split=0.2, verbose=0,\n",
        "                 epochs=200, batch_size=200,\n",
        "                 callbacks=[ck])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GPgNesxQw4M8",
        "outputId": "3308c375-cb16-4af0-a6df-6ba687bab2f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.8436\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.4471937119960785, 0.8435754179954529]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "best_model_path = \"t_model/titanic_176_0.5217.h5\"\n",
        "best_model = load_model(best_model_path)\n",
        "best_model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ueOCfILBw4Kt"
      },
      "outputs": [],
      "source": [
        "y_acc = hist.history['accuracy']\n",
        "y_vloss = hist.history['val_loss']\n",
        "xs = np.arange(1, len(y_acc)+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "NpsjsEunwA7Y",
        "outputId": "3a9393cc-903c-4f0a-dc94-48914cd6a942"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAHgCAYAAABXfvCOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXiU5dX48e8zk33f95AFEpZsQMKiQABxARVxQ7Rai1WsttVabKt9a622ta9ttVqt7Vv059K6INWiogIKgmEXCCQkgAlkIytZJ+skmZnn98ckQ0ImySQkZOF8rsuL5JlnueeOhJM75z5HUVUVIYQQQgghxDmakR6AEEIIIYQQo40EyUIIIYQQQpxHgmQhhBBCCCHOI0GyEEIIIYQQ55EgWQghhBBCiPNIkCyEEEIIIcR57EZ6AOfz8/NTIyMjL9rzmpqacHV1vWjPG+tkvgZO5mxgZL4GTuZsYGS+Bk7mbGBkvgZupObs8OHDVaqq+lt7bdQFyZGRkRw6dOiiPW/nzp0sWrTooj1vrJP5GjiZs4GR+Ro4mbOBkfkaOJmzgZH5GriRmjNFUQp7e03SLYQQQgghhDiPBMlCCCGEEEKcR4JkIYQQQgghzjPqcpKtaW9vp7i4GL1eP+T39vT05MSJE0N+3/Gqv/lycnIiLCwMe3v7izgqIYQQQoihNSaC5OLiYtzd3YmMjERRlCG9d0NDA+7u7kN6z/Gsr/lSVZXq6mqKi4uJioq6yCMTQgghhBg6YyLdQq/X4+vrO+QBshhaiqLg6+s7LCv+QgghhBAX05gIkgEJkMcI+ToJIYQQYjwYM0HySKqrq+Pvf//7oK699tprqaurG+IRCSGEEEKI4SRBsg36CpINBkOf137++ed4eXkNx7AuiKqqmEymkR6GEEIIIcSoJEGyDR5//HFOnz7N9OnT+fnPf87OnTtZsGABN9xwA9OmTQPgxhtvJDk5mbi4ONatW2e5NjIykqqqKgoKCpg6dSpr1qwhLi6Oq6++mpaWlh7P2rRpE3PmzGHGjBlceeWVVFRUANDY2Mg999xDQkICiYmJfPjhhwBs2bKFmTNnkpSUxJIlSwB46qmneO655yz3jI+Pp6CggIKCAiZPnszdd99NfHw8Z86c4cEHHyQlJYW4uDh+85vfWK45ePAgl19+OUlJScyePZuGhgZSU1PJzMy0nDN//nwyMjKGcKaFEEIIIUaHMVHdoqunN2VzvLR+yO5nNBpJCPfmN8vjej3n2WefJSsri6NHjwLm1onp6elkZWVZqji8/vrr+Pj40NLSwqxZs7jlllvw9fXtdp/c3Fzee+89Xn31VW677TY+/PBD7rrrrm7nzJ8/n/3796MoCq+99hp/+tOfeP755/nd736Hp6cnx44dA6C2tpbKykrWrFlDWloaUVFR1NTU9Pt+c3Nzeeutt5g7dy4AzzzzDD4+PhiNRpYsWUJmZiZTpkxh1apVvP/++8yaNYv6+nqcnZ259957eeedd5g3bx45OTno9XqSkpJsn2whhBBCiDFizAXJo8Xs2bO7lTl76aWX2LhxIwBnzpwhNze3R5AcFRXF9OnTAUhOTqagoKDHfYuLi1m1ahVlZWW0tbVZnrFt2zbWr19vOc/b25tNmzaRmppqOcfHx6ffcUdERFgCZIANGzawbt06DAYDZWVlHD9+HEVRCA4OZtasWQB4eHgAsHLlSp5++mna29t5/fXXWb16db/PE0IIIYQYi8ZckNzXiu9gDLZOsqurq+XjnTt3sm3bNvbt24eLiwuLFi2yWgbN0dHR8rFWq7WabvHQQw+xdu1abrjhBnbu3MlTTz014LHZ2dl1yzfuOpau487Pz+e5557j4MGDeHt7s3r16j7Lt7m4uHDFFVfw8ccfs2HDBg4fPjzgsQkhhBBCjAWSk2wDd3d3Ghoaen1dp9Ph7e2Ni4sLJ0+eZP/+/YN+lk6nIzQ0FIC33nrLcvyqq67ilVdesXxeW1vL3LlzSUtLIz8/H8CSbhEZGUl6ejoA6enpltfPV19fj6urK56enlRUVLB582YAJk+eTFlZGQcPHgTMP0h0blC8++67efjhh5k1axbe3t6Dfp9CCCGEEKOZBMk28PX1Zd68ecTHx/Pzn/+8x+tLly7FYDAwdepUHn/88W7pDAP11FNPsXLlSpKTk/Hz87Mcf+KJJ6itrSU+Pp6kpCR27NiBv78/69at4+abbyYpKYlVq1YBcMstt1BTU0NcXBx/+9vfiI2NtfqspKQkZsyYwZQpU/jOd77DvHnzAHBwcOD999/noYceIikpiauuusqywjxjxgw8PDy45557Bv0ehRBCCCFGuzGXbjFS3n333W6fL1q0yPKxo6OjZRX2fJ15x35+fmRlZVmO/+xnP7N6/ooVK1ixYkWP425ubt1WljstW7aMZcuWdTvm7OzMF198YfX+XccA8Oabb1o9b9asWVZXxMvKyjCZTFx99dVWrxNCCCGEGA9kJVnY7F//+hdXXHEFzzzzDBqN/K8jhBBCiPFLIh1hs7vvvpsTJ06wcuXKkR6KEEIIIWy04m+7eXl77kgPY8yRIFkIIYQQYpyqaWojo1jHntNVIz2UMUeCZCGEEEKIcSqrRAdATkXjsD0jvaiW+/91iFaDcdieMRIkSBZCCCGEGKeySs1Bck1TG1WNrcPyjE0ZpXxxvILDhbXDcv+RIkGyEEIIIcQwOF5azzsHCkd0DNkl9ZaPc8q793z48ngFaTmVQ/aMXbndUzrqmtt4cVsO+vbuK8z6diMvb88lv6rpgp89nCRIHiZubm4AlJaWcuutt1o9Z9GiRRw6dKjP+7z44os0NzdbPr/22mupq6u74PE99dRTPPfccxd8HyGEEEJY94+vT/OrjVlkd6zmjoSsUh2zI30A+Laie5D85MdZ/PK/x1BVddD3N5lUy/s7P+B+e38hL27LZcOhM92Ov3ugiOe/zOH6l3ax8UjxoJ893CRIHmYhISF88MEHg77+/CD5888/x8vLayiGJoQQQohhlN6RfvBqWt6IPF/X0k5hdTMLJ/vj7WJPTpcgubSuhTKdnpK6Fo6cGfziW0F1E01tRqL8XMkure+W0vFpZhkAr+3Kx2A0AWAwmvh/u/NJDPMkLsSTn76fwdoNR2kxDD5QHy4SJNvg8ccf79YSunMVtrGxkSVLljBz5kwSEhL4+OOPe1xbUFBAfHw8AC0tLdx+++1MnTqVm266iZaWFst5Dz74ICkpKcTFxfGb3/wGgJdeeonS0lIWL17M4sWLAXPL6aoq868z/vKXvxAfH098fDwvvvii5XlTp05lzZo1xMXFcfXVV3d7jjVHjx5l7ty5JCYmctNNN1FbW2t5/rRp00hMTOT2228HYPfu3UyfPp3p06czY8aMPtt1CyGEEJeqMl0LJXUt+Lk5simzjJK6vv8tHg6dK7wJoZ7EBrrzbZd0i0Nd8oc3ZZQO+hlZpeZUiwcWRgOw55Q5RsmpaOBkeQPzJ/lRVNPMluxyAD47Zp6Lh66I4d01c3jkyhg+OlLCvlLDoMcwXMZex73Nj0P5sSG7nbPRAKEzYNmzvZ6zatUqHnnkEX70ox8BsGHDBrZu3YqTkxMbN27Ew8ODqqoq5s6dyw033ICiKFbv849//AMXFxdOnDhBZmYmM2fOtLz2zDPP4OPjg9FoZMmSJWRmZvLwww/zl7/8hR07dnRrUQ1w+PBh3njjDQ4cOICqqsyZM4eFCxfi7e1Nbm4u7733Hq+++iq33XYbH374IXfddVev7+/uu+/m5ZdfZuHChTz55JM8/fTTvPjiizz77LPk5+fj6OhoSfF46aWXeOWVV5g3bx6NjY04OTnZPNdCCCHESGtpM+Jkr+n13+qh0rmJ7fc3xvOjd9N5fXc+v75+GgCqqtLSbsTFoWcYpm834mSvHZIxdOYKx4V4MDnInf+ml6CqKoqikF5Yi4uDlsuiffkss4wnrpuGVjPwOcku0eGg1XDjjFCe3XyStJwqVkwP5dOMUjQKPH9bEqv+uY91aXlclxDMurQ8Jvq7smRKABqNwiNXxnLl1EDO5qQPyXseSrKSbIMZM2Zw9uxZSktLycjIwNvbm/DwcFRV5X/+539ITEzkyiuvpKSkhIqKil7vk5aWZglWExMTSUxMtLy2YcMGZs6cyYwZM8jOzub48eN9jmn37t3cdNNNuLq64ubmxs0338yuXbsAiIqKYvr06QAkJydbWmNbo9PpqKurY+HChQB873vfIy0tzTLGO++8k7fffhs7O/Nf5Llz57J27Vpeeukl6urqLMeFEEKI0a62qY1Zz2zj82Plw/6sQwW1ONtrWTI1gOWJwaz/pghdSzuVDa2sfuMgs36/jdLzVpe/yC5n+m+/GJLNdGDORw7xdMLXzZHYQHcaWw2U6vTm8RXWMD3cixtnhHK2oZVv8msG9YxjJTqmBLvjaKdl3iQ/duVWoqoqmzLLuGyiL4EeTty3IJrMYh0vfJlDdmk9axZEo+kSkMeHeqIZ5h9aBmPsRTh9rPgORktDA+7u7v2et3LlSj744APKy8tZtWoVAO+88w6VlZUcPnwYe3t7IiMj0ev1Ax5Dfn4+zz33HAcPHsTb25vVq1cP6j6dHB0dLR9rtdp+0y1689lnn5GWlsamTZt45plnOHbsGGvXruXmm2/m888/Z968eWzdupUpU6YMeqxCCCHExXKosJbGVgMnyuq5LjF4WJ+VXlRLUrgn9loNa1Kj+ehoKU98lMW+09U06NsxmFTe2JPPr647t7r88len0Leb+MUHmWx9JBVPF/sLGkNWiY64UE8AJgeZY52c8ga8nO05UdbADxdNZMnUAJzttXyaWcplE30HdH9VVckq0XFdYggAqTH+fJpZxsYjJeRXNXF/qjkF49bkMF74MoeXvjqFn5sjN84IvaD3dbHISrKNVq1axfr16/nggw8sbZl1Oh0BAQHY29uzY8cOCgv7LvOSmprKu+++C0BWVhaZmZkA1NfX4+rqiqenJxUVFWzevNlyjbu7u9W83wULFvDRRx/R3NxMU1MTGzduZMGCBQN+X56ennh7e1tWof/973+zcOFCTCYTZ86cYfHixfzxj39Ep9PR2NhIXl4eCQkJPPbYY8yaNYuTJ08O+JlCCCHESDhUaF4tLa8f/EKULVoNKtml9aREmKtKxIV4Mn+SH5sySvFxteeTH8/n2oRg3vvmDPX6dgD25VVzrETHnXMmUNnYylObsi3325JVxqI/7yDl99v6/G/uH7bz8dESABpbDeRVNREfYg6SYwPMQfK3FQ1knKnDaFJJjvDGxcGOJVMD2JxVTnvH5jpbFde2UK83EB/qAcCCWHNq6DOfncBOo7A0LggAJ3std18WCcDqyyOGLJ1kuI29leQREhcXR0NDA6GhoQQHm3/6vPPOO1m+fDkJCQmkpKT0u6L64IMPcs899zB16lSmTp1KcnIyAElJScyYMYMpU6YQHh7OvHnzLNfcf//9LF26lJCQEHbs2GE5PnPmTFavXs3s2bMBuO+++5gxY0afqRW9eeutt3jggQdobm4mOjqaN954A6PRyF133YVOp0NVVR5++GG8vLx47LHH2LNnDxqNhri4OJYtWzbg5wkhhBAjobPaRMUwB8l5OpMlCO302xVxfHG8gu9dFomzg5YfpEazKaOUdw8U8cDCiaxLy8PPzYFfXz8NPzdH/ro9l9RYPw4V1PLOgSLiQjy4fJJfH0+Fo0V1/PK/x5ge7sXZhlZUFUsA6+liT5CHEznlDbQbTCgKzJhgHt/ypBA+zSxj7+lqFsb62/w+O7v5JXSsVgd7OhMT4Ebu2UYWT/bH29XBcu7350eiorJ6XpTN9x9pEiQPwLFj3TcM+vn5sW/fPqvnNjaa2z9GRkaSlZUFgLOzM+vXr7d6/ptvvmn1+EMPPcRDDz1k+bxrELx27VrWrl3b7fyuzwP42c9+ZvW+Tz31lOXj6dOns3///h7n7N69u8ex5557zqb0FCGEEGI0aTUYySg2B3XluuENknPrzM0zZk44FyRH+7vxwEI3y+fxoZ7Mm+TLG3vymTfRj53fVvLoVbE42Wv58RWT+OrkWX76fgYAP0iN5tGrJ+Ng13cCQJmuhatfSONn/8ngmo5V3PiOABYgNsidnLMNVDe1ERvgjqezOZ1jYaw/7o52bMooHViQXKrDTqMQG3guLlgQ40/u2Uau70jB6OTuZM8jV8bafO/RQNIthBBCCDHuZZfW02YwEezpNOzpFqfqTMQEuPWbU3x/6kQq6lu5/9+HcLbXctfcCADstRpeWDWdBTF+vPX92fzy2qn9BshgXsn97Yo4DhbU8vJXp/B3dyTQ41wVqtgAN3IrGkkvqmVml1VuJ3stV8cFsTW7nFaD0dqtrcoqqScm0L1b+sTKlDCumBLANfFBNt9ntJIgWQghhBDj3uECc6rFsvhgGvQGmtuGri7vqbMN7DtdDZg70J2qNZIS6d3PVZAa48eUIHfKdHpWzQrvlp4wKcCNf987Z0AruwA3Tg9laVwQupZ24kM8ur0WG+ROq8FEg95ASkT38S1PMs9LWk731tK96dy0d/4zpgZ78PrqWbg5jv1kBQmShRBCCDHuHS6sZYKPiyVHd6hSLuqa2/jOqwe449X9PPHRMbJL62k2dE+16I2iKPxkSQzujnbcO39ocnUVReGZm+IJ9XJmfkz3AHtyl7SI5POC5HmT/PB2sbe5sUhFfSvVTW3d0jnGmzET5ncWvxaj24X0fxdCCCGGg6qqHCqsJTXGj6CO9IPyej3R/m79XNm/Jz/OpqapjZXJYby9v4iPjpiDzJRIH5uuX5YQzNL4oCGNcXzdHNn1i8XdahEDxASa36+fmwMRvi7dXrPXalgaH8zHR0toaTPi7NB3BYrOTXudP3SMR2NiJdnJyYnq6moJwEY5VVWprq6WLnxCCCFGlTM1LVQ1tjIzwptAT/O/UUNR4eLTzFI+ySjlJ0ti+PPKJN76/myc7DV4OypEnheE9mU4FgHPD5ABXBzsiAlwY060r9VnLk8KprnNyFcnz/Z7/8wSHRrFnF4xXo2JleSwsDCKi4uprByaDjRd6fV6CeoGoL/5cnJyIiws7CKOSAghhOhbZ33klEjvcyvJutYLuufZej1PfJRFUrgXDy6aCJirRGxfu4htX+8atb/9/te9s3HupU7xnChf/N0d2ZRR2m+zlfTCWqYEeVhtrT1ejIl3Zm9vT1TU8NTV27lzJzNmzBiWe49HMl9CCCHGmsOFtbg72hET4I5Wo+DuaHdBK8kldS388J10WtqMPL8yCTvtuV/Me7rY4+s8en9RH+zp3OtrWo3CdQnBvPtNkbkroFHlVx8do6XNyBv3zLacZzSpHCmq5Zbk8b0oNiaCZCGEEEKIwTpcWMuMCG+0HSkIgZ5Og964tyWrjF98kIlJhb/ePp1JARee1zyaLE8K5s29BTz/RQ5bssot5fKKa5sJ8zankJwsr6epzdhj8994M3p/1BFCCCGEuEC6lna+rWgguUu1iSCPwdVK/tOWkzzwdjpRfq589vB8lsb3nZIwFs0I9ybUy5k39xbg7KDlxVXTAdiVe640XGfnQgmShRBCCCHGqB0nz6KqMG+Sr+VYoIfTgNMtvjpZwd93nub2WeH854HLifB1HeqhjgoajcJjy6Zw3/woPn1oPiumhxDs6cSu3HP7wg4V1hLo4UioV++pG+OBpFsIIYQQYtzalFFKsKdTt7rFQZ6OnG1oxWhSLSkYfaltauOxD48xJcidp1fE2dT9biy7ISmEG5LOtZVeEOPHlqxyy3wdLqwlOcJ71G5OHCrj+6sshBBCiEuWrrmdtNxKrk8M7lYSLcjDCaNJpbqx/woXqqryxEdZ1DW38ZfbpuNo13f94PFoQYw/9XoDmcV1VNTrKa5tITnCtjrQY5kEyUIIIYQYl7Zml9NuVFneZVUUzOkWgE15yZ9klPLZsTIeuTKWaSHjtyZwX+ZN8kNRIC2nisOXSD4ySJAshBBCiHFqU2YpEb4uJJzXOjnIs7NWct9BstGk8r+fnyQp3IsfpEYP2zhHOx9XBxJCPdmVW8mhglqc7DXEXQI/MEiQLIQQQoxDl3qX2qrGVvacquL6xOAeubOdDUX627x3sKCG8no9986P6lYL+VK0IMaPI2fq+DrnLIlhXthfAvMx/t+hEEIIMUhpOZXEPbmF3IqGkR7KgHxwuJhZz2ynpc040kMZMZuzyjGp9Ei1APB1c0SrUfpNt9iUUYqzvZYrpwYM1zDHjAUx/hhNKqcrm0i5BFItQIJkIYQQolcvbc+lqc3IP9PyRnooNjMYTfx1ew5Vja0UVDeN9HBGzKaMUmIC3Jgc6N7jNa1GIcDdsc/W1Aajic1Z5SyZGjCuWy/bauYEb1wczJsWL4V8ZJAgWQghhLDqcGEthwprCfZ04uOjJYPu0Haxbcku50xNCwCF1c0jPJqLp7SuhRtf2cPSF9NY+mIaBwtquD4xpNcyZf3VSt57upqapjarK9GXIgc7DZdFm2tNdy2nN55JkCyEEEJYsS7tNJ7O9rxxzyyMJpU39uaP9JD6paoq69LyLE0eimounZXkz4+VcfRMHeE+LkT4urA8MYQ7Zof3en5/Xfc2ZZTi7mjHwlj/4RjumPTDxZN4bOkUvF0dRnooF4UEyUIIIcasrdnl7D1V1f+JA5RX2cgXxyv47twIpgR5sCwhmHf3F9Ggb7f5Hqqq8saefAqqLl6guj+vhsxiHT9aPAkvF/sRX0neklVOWk5l/ycOgbTcKiYFuPHq3Sn887spvHTHDAI6NuhZE+TpREUvvx1oNRjZkl3OVXGBONlfenWRe5Mc4c2DiyaO9DAuGgmShRBCjEl1zW08sv4of9h8Ysjv/drufOy1Gr53eSQAP0iNpqHVwPpvzth8j+Nl9Ty96Tgvbc8d8vH1Zl3aafzcHLh5ZigRPi4U1YxckKxrbmfthqOs3XAUffvwbiDUtxs5kFfNghg/m68J9HCiodVAU6uhx2u7cqpo0Bsk1eISJ0GyEEKIMent/YW0tBs5UdZgNdAZrKrGVj44XMwtM0Pxd3cEIDHMi7nRPry+J592o8mm+3yaWQbAF8crhj1IBMipaGDHt5V877JInOy1TPB17XMl2WRS2XuqCpPJ9lJxmcV11DW32XTu2wcKaW4zUtXYxn/TS2y65my9nsOFNT2O61ra2XuqqteydocKamk1mEiNsT01IsjT/LXtTLn4Jr+GjUeK2XikmDf25uPlYs/8SbYH3WL8kSBZCCHEmKNvN/Lm3gL83BwwmlQyztQN2b0/PlpKm8HEvfOjuh1fsyCaMp2e7Scq+r2HqqpsyijFz82RxlYDO78d/pSDjUdKsNcq3DU3AoAIHxdK6lp6Deq/zq3kO68d4B9fn7bp/qfONrDilT1c/UIae0/3neKibzfyxp4CFsb6kxDqyWu78voNxr88XsHVL6Zxyz/28auNxyw/WBwsqGHZi2l857UD/PCddHTNPVNe0nIrcdBqmBNte6vkzq57BVVNPP5hJrf9cx8/fT+Dn76fwZ5T1axICrkkagGL3slXXwghxKjU3GbodeXwv+klVDW28cxNCSgKHOpolTsUduVWEu3vyqSA7qXDFsb64+vqwKaOFeK+HD1TR3FtCz+/JrbjmtIhG5/RpFpdmc4q0TE5yN2yqWqCrwtGk0pJbYvV+2Se0QHw4rYcjpfW9/vcV9PycdBqcHOy487XDvDc1m8x9BKAf3SkhKrGVn6QGs39qdHkVTXxZZcfLtqNJs7UNHOmppmi6mae+iSbNf86RKiXM6svj+SdA0Xc8Lfd/O/mE6z65z7stBoeXDSRL49XsOyv5soVXaXlVJIS6T2gUm2dDUV+/O4R3j90hgcXTWTHzxaxs+O/X18/zeZ7ifFJCv8JIYQYdQxGEwv/vJOrpwXyzE0J3V4zmVRe25VHQqgnV08LJDbAncNDFCS3Gozsz6vm9lkTerxmp9WwLCGIDw4X09RqwNWx939CN2WU4aDVsCwhmGMlOpuusYWqqvz43XROVzbyxU8Xdjt+rETHsvggy7EIHxcACmuaifRz7XGvYyU6Qr2caTOaWLvhKB//eB6OdtY3qdXpTWw8UsLKlDB+dd1Unvokm7/tOIWiwKNXT+52rsmksm5XHvGhHlw20RejSSXM25l1aXlcExdEdqmOh947Ql5l9w2N358XxWPLJuNop2XxlAAe3XCUf36dx4rpIfz+xnjcnexZGhfEQ+8d4Y51+/nPA5cxY4I3Z+v1nCxv4LGlUwY0l0GeTthrFdyc7Hj1thTmDyCfWVwabFpJVhRlqaIo3yqKckpRlMetvD5BUZQdiqIcURQlU1GUa7u89suO675VFOWaoRy8EEKI8elkeQOVDa28c6CIr052T2/48kQFeVVNrEmNRlEUkiO9SS+qHVBubW8OFdSibzeRGms9YFqeGIK+3cS2PlIuTCaVz46VsmiyPx5O9pZrtp88e8Hj++BwMZuzysmpaOxWt7mkroW65nbiQjwtxzoD46JeGopkl+pIifTmj7ckcLK8gRe+7H2D4bYiA+0mE/ctiMbFwY4/3ZrEwlh//pte0mO1f/vJs+RVNrFmgfnrY6fVcN/8KA4X1vLkx1nc9MpeGvUGfrcijj/fmsifb03kwwcv58nl0yxB+sJYf7Y8ksq7983hxVXTcXeyByAp3ItNP55PoIcTj27IoKXNyO6O6iYD2bQH4OJgx8YfzmPrI6kSIAur+g2SFUXRAq8Ay4BpwB2Kopz/O4gngA2qqs4Abgf+3nHttI7P44ClwN877ieEEEL0Kr3IvDIc5u3MLz44Rk2TebPYmZpmntv6LWHezlzbsWqaPMGbBr2B3LONF/zctNxK7LUKc6J8rb4+K9KHQA9Hy6Y8aw4W1FBR38r1HZUROq/ZlHFhKRfFtc38dtNxwrzNNZC7rp5nlZjTJeJDzwXJAe6OONlrrG7eq2pspUynJz7EkyumBHL7rHDWpZ3mZHnPtIvGVgNfFbVzzbQgorqsSN+QFEJJXQvpRd3zwdelnSbUy5nrEoItx26bFY6Xiz3/2lfIghg/tjySyncvi2RlSjgrU8KtdnDzc3Pk8kl+PWNq3jQAACAASURBVJqBeLrY8+dbE8mrauKPW06yK7cKX1cHpgV79Dl/1sSHeuJzidT8FQNny0rybOCUqqp5qqq2AeuBFeedowKd/3d6Ap3fCVYA61VVbVVVNR841XE/IYQQoleHCmoJ9HBk3XdT0LW08euPsvgss4xrX9pFuU7P72+Mx65jU1VKpDnAOmSlKsJA7cqpIjnCu9e0CI1G4bqEEL7+thJdi/WayZsyS3G213Ll1ADLNdcn9n1Nf0wmlZ//JxOTqvLve+fgZK/p9n6zS3VoNQpTgs7lUSuKwgQfFwqtlIHL7shBjgs1/9O99qpYTCrsOVXd49z3D56h2QD3L4zudvyquEAc7DR82iXfOr2oloMFtdw7P8ry9QHzqu0Lt03nz7cm8tr3Ui44ML18kh+rL4/kzb0FbMkqZ36MHxqN9c56QgyWLUFyKNC1MGRxx7GungLuUhSlGPgceGgA1wohhBDdHC6sJSXCh2khHjxyZSyfHSvjR++mM9Hfjc8eXsCiyQGWcyf4uODn5nDBecmVDa0cL6tnQT9lxJYnBdNmNPFFdnmP1wxGE5uPlXPF1IBum8iWJ4X0es2GQ2f4z6EzvW5SBHjnQCH78qr59fXTiPJzJSnMi/RuK8k6YgLcejS+mODjSpGVleSsEvOmvc70jAAPJwI9HMnuON71/by+O59Yb02PVsQeTvYsivXns8wyjB2pLuu+zsPT2Z5Vs3p2uls8JYCVKeG9tokeqMeWTiHa35WWdmO/XzMhBmOoNu7dAbypqurziqJcBvxbUZR4Wy9WFOV+4H6AwMBAdu7cOUTD6l9jY+NFfd5YJ/M1cDJnAyPzNXDjbc5q9SZK6lpIDTKyc+dOpqgql4fY4eussGJiG3nHviHvvGsiXI3sPlnKzp39l4Lrbb72lpprLbvWF7JzZ3Gv16uqip+zwr92ZuPf2L18Wm6tkeqmNiKUmm7PUFUVT0eFj/Ye73aNSVV5clszeiOs35XNPXGOuDn0DCJf39dClKeGwKbT7NyZh7/SxuaSdrZu34GjViG9oJl4X22P96VtbiW/ysCOHTu6Bac7juoJcFE4cmCP5ViQo4EDud3nsEBnpKROz/diVatzNtHewBcNrazb+BXeTgpbs1u4Ltqeg/t29zp/Q+nuSUY2mDQ4Veeyc+epi/JMW4y3v5MXw2icM1uC5BKg64+EYR3HuroXc84xqqruUxTFCfCz8VpUVV0HrANISUlRFy1aZOPwL9zOnTu5mM8b62S+Bk7mbGBkvgZurM1Zm8HEl8crWDTZ32paw2eZZUA6ty+ZRVK4FwBLruj7njma0/zh85PEJV9maQDSqbSuhaKaZuZGm/OMe5uvT94/io9rJXcvv6LfX92vbD3JurQ8Emdd3i114MiXOWiUXO5fkYqXS/eUguTCg5TUtrBoUarlWH5VE/qtO5k/yY/9edX84bCJf9yVbHnfAC1tRs58sZX7U6NZvNhcwcEUVMGneYfwiExkor8rui3buTI5lkXzutd2LnIsYGthNnHJl3Vr0fzkNzuYNdGTRYtmWo6lt+fwt69ymX35fMsq+Jt78oHjJAa7WJ2z2W0G3jy+jTOaAIrbwF5bzJN3pBLg3ns76KG2+qI9yXZj7e/kaDAa58yWdIuDQIyiKFGKojhg3oj3yXnnFAFLABRFmQo4AZUd592uKIqjoihRQAzwzVANXgghxNhSVN3Myv/by4/eTeeJj7KsnnOosAYnew3TQmzfiJUcYW4iYS3l4s9bv+X2dfv5+X8yaG6z3plPVVXScquYP8m23NbliSEYTSqbs7pv4NuVW0limFePABkgPsSD3LMNtLSdq3Hcmfbw+LIpfPjg5ZhUePKT7G7XZRTXYTCp3Ta3daY+pBfVklVqvkfXTXudJnSUgSvoknKha26nqKbZko/cdXwmFU6UNViOHSqsJdjTCV9n6+GCi4MdS6YG8FlmGR8cLubmmaEXNUAWYjj1GySrqmoAfgxsBU5grmKRrSjKbxVFuaHjtEeBNYqiZADvAatVs2xgA3Ac2AL8SFXV4e/NKYQQYtT5JKOUa1/aRX5VE8vig9h4pITNx3pWiUgvrCUpzGtA3c7iQz1wsNNYbWmcV9mIl4s9H6QXc/3Luyms7/nP0MnyBqoaW20uIzY12J1of9duFSt0ze0cPVNHaqz1/Ni4UE9MKt0qSGSV6rDXKsQGupMU7sXts8N7tH7uDPy75gR7uTgwKcCNQwU1ZJXUoygw1Up1hwhfczWKwi5l4LI7g+qQ7kF1Z5Dd+TqYvxbWKk90tTwpBF1LO60Gc4k4IcYLm74Dqar6uaqqsaqqTlRV9ZmOY0+qqvpJx8fHVVWdp6pqkqqq01VV/aLLtc90XDdZVdXNw/M2hBBCjGbHinU8/N4RpgS58/lPFvDSHTNICPXkfzYe42zDuXq/LW1Gskvr+w3MzudopyUh1JOjVtpTF9Y0c21CMO/cN4emVgN//EZPg757lYktWeYNdbZuAFMUheWJIRzIr+FsvXn8e09XYVIhtZdAuzMIzerS3S67pJ7JQe442Gksz1dVLLV/wRwkT/R3tXTS65Q8wZv0ojoyi3VE+bniZiV1JdTLGY0CRV0qXHSuPMedt1If7OmEj6uDZXW7tK6FUp2+36/Fwlh/PJ3tuWpaIJMC3Po8V4ixRNpSCyGEGHbbT1agKPDq3SmEebtgr9Xwl9uSaGoz8j//PWap7NCZWtBZ1m0gYgPdOX1eFzddSzt1ze1E+Lhw+UQ/Xrt7Fs0GeO+bIss5+nYjb+8vZMmUAII8bU8VWJ4UjKrCZx2r4Wm5Vbg72nXLJ+4qxNMJbxd7SwUJVVXJKtWR0CVNIinME3cnO3blmINkk0klvchc6eN8yZHe6FraScut7LEq3MnBTkOIl3O3WslZJfWEeDrh69Y9d1tRFOJDPS01lztbfVt7dldO9lo++fE8nluZ1Od5Qow1EiQLIYQYdrtyq0gM8+q2GhoT6M4vrpnMthNn+d2nJ2gzmKymFthqor8rNU1t3VIVOsufdaYdJIR5MtVHw+u7C2gzmABzF7vqpjbuTx1YqsCkAHemBnuwKaPUnNOcU8llE317TRPpDEKPdQTJ1rrk2Wk1zJvox67cSlRVJa+qkbrmdquruZ3H2gwm4kN7z9+O8O1eKzmrVEeclfxlMOcl51Q00Gowkl5Yi7O9linB7lbP7f4MVzyd7fs9T4ixRIJkIYQQw0rX0pGrayUN4fvzovju3Ahe35PPyv/by5fHK5gU4GZ141t/ov3NgXDX1eTCGvPHEb4ulmPLouwpr9ezKaMUo0nltV15JIV7MTuq7xVTa65PDCa9qI49p6rNZet6yUfuFBfiaQlCrXXJA0iN9adUp+d0ZZPlh4ZkKyvr0X6ueLvYW71HV+ZayeZ5aGw1kF/V1OvKc3yoJwaTSk55I4cKa5gePrDccCHGE/k/XwghRK/2nqriL1/mXNA99p2uxmhSreb7ajQKv7sxnn/cOZP8qiaOnqkjeRCryADRfuZ82LzKc+2pO9MMOqs8ACT4aZkc6M6ru/L4IrucgupmfpAaPagmF8sTza2nf/2xuVJHaj85zfGhHrQbVXIrGq12yQMsmwd35VZyqKAWbxd7oru0g+6kKIplNTmul6AXINLXhdrmdu5Yt5+7XjuAqkJCmPWV587g+UB+NSfKGgacGy7EeCJBshBCiF69vqeAV3acsnRUG4y03EpcHbTMmGA9VxdgWUIwmx9J5ZaZYdw5d8KgnhPm7Yy9ViGvqstKcnUTfm6O3eoxK4rCmtRoTpY38MRHWUT4unBNXNCgnjnB14WkcC/yq5qI8HVhQpcVa2s6g9CsEl2vXfLCfVyI8nMlLaeSw0Xm6hK9BfDfuzySHyyM7jPVYcnUAC6f6IvBZMJeq7BkSgCzIq2vmof7OOPuZMe73xRhNKlWV7CFuFQMVcc9IYQQ44yqmjeNGU0qVY2tBHr0v6mtqdXAxiMl3JYSjoOdpkuurl+/v7YP9XLm+dsGv/nLTqshwte1x0pyhJXA9YakEJ7b+i3l9XoeuTIGrQ21kXuzPDGYjDN1NpWPm+DjgrujHVmlOrJK63tdeV4Q48f7B8/QajBxa3JYr/dbEOPfb0WOSQHuvLtmbr9jg4686RBP9uVVAzAzXIJkcemSlWQhhBBW5Vc1UdNk3gRXrtP3c7bZB4eLeeKjLF7+KhcwB6nFtS2kxtpWf/hCRfm5ktclJ7moppkIn55BsoOdhoeXxDApwI1bk8N7vD4QNySFEOrlzA1Jof2eq9EoTAvxYMfJSiobWnvdcJca409rx8bC/qpLDLXOMcUGuuHpIpvxxKVLVpKFEEJYdahL97ryej22rPHuyq0E4O87T3PFlABLzd3+cnWHSrS/K19/W4nRpNJuNFFer+81BeI7cybwnTmDS+3oKsDDiT2P99M3u4uEUE8O5JubnvS24W7uRF/sNAqKAolhvecbD4fOMUk+srjUSZAshBDCqvTCWhy0GtqMJirq+19JbjOY2He6mhXTQziYX8OjGzII9XYm3MfZasrDcJjo50ab0URxbTPtRhOqykV7tq06g1BFgWlWuuQBuDnacdlEX9oMph45y8NtRrg3Wo3CvEkXZ/VfiNFKgmQhhBBWHS6s5bKJvuw5VWVTukV6US1NbUauTQjmtpRw7nztAHlVTXxnzoRBVY4YjM4ycHmVTZg6GpRM8OlZGWIkdaYzRPu5dttQeL5X7pyJOvj9koM2wdeFtF8sJmQAjVWEGI8kJ1kIIUQPdc1t5J5tZHaUDwHujpTbsJK8K7cSrUbhsom+zJvkx+rLI4He2zQPh2h/cxm405WNlvJvo20lOcrPDVcHbbdOe9Z4ONmPWIOOUC/ni/aDjRCjlawkCyGE6OFIUR1g7ny37USFTekWu3KrmDnBCw8nc2D3+LIpJIV7cuXUwGEda1c+rg54udiTV9WEg1aDq4MWX9eBNyYZTlqNwuurZxHi5TzSQxFC9EFWkoUQ4hL1/3bn8/L2XKuvHSqsQatRmB7uRZCHU7/pFjVNbRwr0XUrR+Zkr+WmGWHYXeSObdF+5jJwhdVNTPB1HZUronOifQm3UnVDCDF6yEqyEEJcgs7W6/nj5pO0GU1MDnLn6vOaaRwurCUuxANnBy2BHk7syq3q8357TlWhqthUK3i4Rfu7kZZTiZujHbGB7v1fIIQQVshKshBCXILe2FuAwWQi2t+VX/73GFWNrZbX2o0mc3vojhJgQZ5ONLYaaGw1WM6pqNfzaloeTR3HduVW4uFkR2JY7131LpZof1fONrSaaySPsnxkIcTYIUGyEEJcYhpbDby9v5Cl8UH8485kGvQGfrXxGGpHKYUTZfXo203nguSOTntdUy7e2V/IM5+fYPnLu8kq0ZGWU8X8GL8L6lw3VKL9zJv3DCaVCN/RVdlCCDF2SLqFEEJcYtZ/U0SD3sD9qROZHOTOz66J5Q+fn+SFL3OYGODGvtPmlsSdQXJglyB5UoA5AP22ogE/N0ea2gzc+MoeDCa13/bIF8tE/3OBsawkCyEGS4JkIYS4hLQbTby+O5/ZUT5MDzenRtw7P5qd31by0lenLOdF+7sS7GmuvhDUUS+3axm4nIpGZkd58/sbE/jFBxnsPlXFwtjRESRP8HVBo4BJhQmyOU4IMUgSJAshxBjWoG+nusVESV0LChDs6dRnNYfPMsso1en53Y3xlmNajcK/vj+bwppmy7EAd0fLx53pFp1l4PTtRgqqm1gxPQQfVwdevTuF5jZjn40xLiZHOy3hPi6U1rVImTUhxKCNju9oQgghBkzfbmTes19RrzfA118B8OhVsTy0JKbXa97eX8hEf1cWTw7odtxOq2FiRyOO8zk7aPFwsrPkJJ8624iqwuSOyhGKooyaALlTbKA7DlrNqMiRFkKMTbJxTwghxqjC6mbq9QaWTLDjT7ckMjnQnS9PVPR6frvRRGaJjsWTA9AMMHgM8nSypFt8W94AQMwoLq/29A1x/OOumSM9DCHEGDa6fvQXQghhs7zKRgBSw+y4bVY45fV6XtiWQ01TGz5WusydrmykzWAivp92yNYEejhZ0i1yKhpw0GqIHMWb4iTNQghxoWQlWQghxqi8qiYAglzM38oXxPihqrD7lPXGH8eKdQCDCpK7dt3LqWhgYoDbRe+kJ4QQF5N8hxNCiDHqdGUjwZ5OONqZUycSw7zwcLJjV06l1fOzS+txcdAS5Tfw2sFBnk5UNbZiMJrIqWhkcqD1/GUhhBgvJEgWQogxKq+yieguNYG1GoX5MX7syq2yNAbpKqtEx7Rgj0FtZgv0cMKkQn5VEyV1LcQGjd58ZCGEGAoSJAshxChnMqlszS5H3260HFNVlbzKxh6rwgti/Cmv13PqbGO340aTyvGy+kGlWsC5MnBpueZUjsmjeNOeEEIMBQmShRBilHtrXwE/+PdhPjhcbDlW3dRGvd5gacHcaUGMH3AumO2UX9VEc5uRuBCPQY2hs6FIWkcqR6wEyUKIcU6CZCGEGEXONui7fX66spFnN58E4HBhreV4XqV5017XdAuAMG8Xov1d2ZXbPS85u3Twm/bgXGvqA/nVuDpoCZXqEUKIcU6CZCGEGCU2HDrD7Ge2s3bDURpbDRiMJtZuyMDZQcvsSB8OFdZYzu0s/2atAUhqjD/786ppNZxLz8gq0eFopyEmYHAb7nxdHbDXKujbTUwKdB9wnWUhhBhrJEgWQohR4ExNM09/kk24jzMfHSlh+cu7+fXHWWScqeN3K+K5Oi6QMzUtnO2oVZxX1YSDncZqPeAFMX7o200cLji38pxVUs+UYI9Bl23TaBQC3M2ryVLZQghxKZAg2RpVhbozcGob7P8HnPlmpEckhBjHTCaVn/0nA0VReG/NXN5dM5eWNiPvfXOG5UkhLE8KITnCGziXcpFX2UiUr6vVShVzo32x1yp8eqwMMG/yyyrVET/IfOROgR6OgOQjCyEuDdJxr6uqXDj6DmSsh4ayc8fDZsF920ZuXEKIce31PfkcyK/hT7cmEubtQpi3C5t/soAP04tZmRIOQFyIJ452Gg4V1rIsIZi8yiYm91KGzdXRjlWzwnnnQBErkkII8nSiQW8YdD5yp87Ne709VwghxhNZSQYwGeGN6+BvKbDnrxCUCNc9D6s/gynXQ0PFSI9QCDEG7T1dxdObsq3WLO6UV9nIn7Z+y5VTA1iZHGY57u3qwH0LovF0tgfAwU5DUpgXhwtraTeaKKpp7rFpr6tfLpvKBB8XHv1PBgfyzLnM8SEXFiR3bt6T8m9CiEuBrCQDaLQQnASxV0PiKnAPOvdazlbI/cKcgqHIRhUhhG2qG1t5+L0jVDW2ceecCCb1smHu7ztPo1HgDzcnoPTzPWZmhDf/b3ceORUNGExqj/JvXbk62vH8yiRW/nMfv/30OHYahdigC8slvi4hGJNJxd/d8YLuI4QQY4GsJHda+geY95PuATKYPze2QUut9euEEOI8qqryPxuPUdfcDtCjHFuncp2ej4+WcFtKuGVTXF9SIrxpN6p8fLQU6Fn+rcf5kT7cnxpNY6uB2EB3HO20A3wnPe/39Ir4foN5IYQYDyRI7o9boPnPxrMjOw4hxJix8UgJW7Mr+Nk1k4nyc7U04DjfG3vzMZpU7psfbdN9Z3Zs3vtvegkA0VbKv51v7VWxJEd4c+W0QBtHL4QQAiTdon+WILkcAqaM7FiEEKNeaV0Lv/kkm5QIb9YsiKa0roX/HCqm1WDstpLboG/n3f1FLIsPZoKvi0339nF1INrflbzKJvzcHCz5yn1xtNPy4YOXD/r9CCHEpUpWkvvTmX4hK8lCCBu8tD2XdqOJ529LQqtRWBDjT0u7sVu3PID135yhodXA/am2rSJ3Sp5gXk3uKx9ZCCHEhZMguT9uAeY/G8pHdhxCiFGvzWDi82NlXBsfTISvOV/4som+2GkUduVWWc5rN5p4fU8+c6J8SAr3GtAzOusl95ePLIQQ4sJIkNwfRw+wc4ZGKQMnxHhiMqkseX4nr6blDdk9d+VWUq83cH1SsOWYm6MdMyO8u23eW3/wDGU6PT9YOLBVZDBvngN6rZYhhBBiaEiQ3B9FMa8mS5AsxLiSe7aR05VN/H3nKVrajENyz00ZpXg62zN/kn+346kxfmSV1FPV2EpRdTP/+/kJ5k3yZfHkgAE/Y1KAG/9310xumxU+JGMWQghhnQTJtnAPknQLIcaZQ4XmBhu1ze385/CZC76fvt3Il8crWBYfhINd92+tC2LMQXNaTiU/+08GWo3Cn29NGnQptaXxwXg49b9pTwghxOBJkGwLt0DZuCfEOHO4sBY/Nwemh3vx2i5zKbYLsePkWZrajCxPCunxWnyoJ14u9vz+sxN8U1DDU8vjCPFyvqDnCSGEGF4SJNvCLdBcAk4IMW4cLqxl5gRvfpAaTVFNM1uyLuzv+KbMUvzcHJgT5dPjNa1GYf4kP2qa2rh6WiA3zwy9oGcJIYQYfhIk28I9EPQ6aNeP9EiEEEOgsqGVwupmUiK9uTouiEhfF9alnUZVB7ea3NhqYPuJs1ybEIyd1vq31VtmhhEf6mFT+2khhBAjT5qJ2MLSUKQCvCNGdixCiAvWWbM4OcIbrUbhvgXRPPFRFgfya5gb7WvTPcp0LRwqMN8nq1RHq8FkNdWi0+IpASyeMvCNekIIIUaGBMm2cOtsKCJBshDjQXpRLQ5aDfGhngDcmhzGC1/msC4tz+Yg+RcfZHarfTzBx8XS6EMIIcTYJ0GyLdy7rCQLIcaENoOpR5WJTocKakgI87S0iXay13L3ZZG8sC2H3IoGYgLd+7x3S5uRA3k13D4rnPsWRAHg7+6ERiNpFEIIMV5ITrItOtMtpAycEGNCbkUDSU9/wdoNR2lsNXR7Td9uJKuknpSI7qu+370sAid7DetsaC5yIL+aNqOJZQnBTApwZ1KAO57OUpJNCCHGEwmSbeHqD4pGysAJMUb8My0Po0nloyMlLH95N8eKdZbXskp0tBlNzDwvSPZxdeC2lHA+OlpCRX3fm3R35VbhYKexWslCCCHE+CBBsi00WnDxkzJwQowBFfV6Pj5awu2zw3lvzVz07UZu/sceXtuVh8mkdtu0d7775kdjNKm8sacAMFfB+NG76by8PbfbebtyK5kT5YOTvXbY348QQoiRITnJtnIPhAbJSRZitHtjTwFGk8p986OZ4OvC5w8v4BcfZvL7z06w+1QV+nYjUX6u+Lk59rh2gq8Ly+KDeedAIUlhnvz64yyqGtv4UlvBqlnhBHg4Ua7Tk1PRyK3JYSPw7oQQQlwsspJsK7cg2bgnxCjX2GrgnQOFLIsPZoKvCwDerg6s+24yv1sRx97T1ezPq2FmH1Uo7k+NpkFv4MF30vFxdeCf302m3WTirX0FAKTlVgLnWk0LIYQYn2Ql2VZugVCRNdKjEEL0Yf03RTToDdyfGt3tuKIofPeySFIiffjD5ydYmdL7KnBSuBd3zZ2AvVbDY0un4GSv5ZppQby9v4gfLprErtwq/NwcmRLUdwUMIYQQY5tNQbKiKEuBvwJa4DVVVZ897/UXgMUdn7oAAaqqenW8ZgSOdbxWpKrqDUMx8IvOPdC8cc9kAo0swIsLc6xYh7erPWHeLgO+9my9njO1zSRHjJ5NY5nFdXi7OBDu0/395FY0YFJhso0BZZmuhaNnDSxU1QF3pWs3mnh9dz5zo31ICveyes7UYA/+fe+cfu/1+xsTun1+/8JotmSX8943RezOrWTx5ADpmieEEONcv9Geoiha4BVgGTANuENRlGldz1FV9aeqqk5XVXU68DLw3y4vt3S+NmYDZDCvJKtGaK4e6ZGIMU5VVe596yC/+CBzUNc/vek4d6w7gK6lfYhHNjhGk8rqNw7y43fTu7V1NhhNrH7jIPf/+5BN7Z63ZJVxzQtpvJjeyoNvp1PX3Dagcfzz69OU6vQ8sHDigN9Df2ZO8GZWpDcvfJlDbXM7C2L9hvwZQgghRhdblkRnA6dUVc1TVbUNWA+s6OP8O4D3hmJwo4qlNbWVChftLVIeTtjsbEMrZxta2ZdXzdnzSo01txloaTP2em1jq4HtJytoM5r4Intoq60U1zZzoqyeE2X15hVgU/+BLUB2qY6apjYyinUcyK+xHP/sWBkldS0UVjdzrETX6/X6diO/2niMB95OJ8rPlRsn2bPtRAXX/nUXX52ssIyp639napq73SOrRMeL23JZnhTCosnD0/r5/tSJNHV8beZPknxkIYQY72xJtwgFznT5vBiw+vtKRVEigCjgqy6HnRRFOQQYgGdVVf1okGMdWe5dWlPT/VexbP8tHPsA1p4AraR5i75ldQSMqgqfHytj9byojs9Vvvf6N+RXNfH8bdNZGNszENt+ogJ9uwknew2bMstYmRI+JGM6dbaBq19Io2tc/Ovrp3Hv/Kh+r+1szezpbM8/vz7N3GhfVFVlXVoeE3xcKNO1sCmjlMQw6ykQz3/xLe8cKOIHqdE8evVk9u5O455rZvPw+iN8/81DvT73e5dF8MtrpwLw6IYMfFwd+N2KuAG864FZMiWASQFuONtr8XfvWRlDCCHE+KL092tQRVFuBZaqqnpfx+ffBeaoqvpjK+c+BoSpqvpQl2OhqqqWKIoSjTl4XqKq6unzrrsfuB8gMDAwef369Rf4tmzX2NiIm5tbv+c5tZQx98ADnJz8MOXBS7q9NvvAA7i0lJE+41nqPacO11BHBVvnS5xz/px9fKqNj061E+Ci4OGg8Ku5zgB8W2Pkf7/R42oPTe2wNNKeW2PtsevS6viv6XoKdCYuC7FjS0E7Ly52wcPhwnNjP8xt49PT7fwg0RE7DWzOb6e2VeVPqc7dnm/N/x5oQW+EmQFaNp5q55l5zujaVP50UM89cQ6knzVypsHEcwud0VjJ4308rRl/Fw2Ppjh1m68Wg8qJaiPW1JTfEQAAIABJREFUFrS/rTXyZaGBcHcNE9w17Ck18NNkR5L8h/eH1Fq9CZMKvs6ja1+C/L0cGJmvgZM5GxiZr4EbqTlbvHjxYVVVU6y9Zsu/KCVA1+WqsI5j1twO/KjrAVVVSzr+zFMUZScwAzh93jnrgHUAKSkp6qJFi2wY1tDYuXMnNj2vrQkOPMCUMG+mLOhyfl0R7CwDYKZ7DVzEsY8Em+dLWJw/Z28XHiLav5GbZ4bx563fEjN9DqFezvz7zYP4uJr46tGFPPfFt7y9v4h6rXmjmVajoGtuJ+vLL/neZZHcPDOMz1/aRYNnNDfMibig8amqylMHdzJvkiePf8f8S6Lkk2e5582D1HvFcPNMcyWIBn0733/zILfPmsAtHTWCG1sNnP7iC9akRrNmQTSbn93OEb0vZxta8XNTeez2xWzOKuOn72fgEZVESmT3zYZnapop37KDNVdMZlHHqnXX+VrWx7h3nDzLo//JYE9pG3fMnsBPbk7o4+zxTf5eDozM18DJnA2MzNfAjcY5s2U55CAQoyhKlKIoDpgD4U/OP0lRlCmAN7CvyzFvRVEcOz72A+YBx4di4Bedgys4uPeslZz3tflPt0A4vf3ij0uMOdmlOuJDPbk+MRiAzzJLya1oYPvJs9x9WQReLg78/sYEnrkpnr2nq3ltVx4AW4+X025UuT4phKnB7kz0d2VTRukFjyerpJ6C6maWJwVbji2a7E9soBvr0vIsm+5+u+k4Bwtq+dPW/8/enYdXVZ3tH/+uk3kOITMEQiBhkkHmWRBFUMEZcUQUsbW+b21rW/1ZW6vW2tfWtlZbpWq1VqXOCoIjIGGUSeYhEGYSpkAGMufs3x87hCQkkJPk5GS4P9eVK5x91tn7Ofui7d3Fs9faTnGpE4CVu09Q6rQYnRx5dlvn9YdYsvMYM0Ym4u/jxWU9Y/DzdjBvY8Y51166y27VuKQeD8KN6xHN5z8ezaNX9uRXV7Xuf8EREZGmd8GQbFlWKfAA8AWwDXjXsqwtxpgnjDGVV6uYBsyxqvZv9ATWGGM2AIuwe5JbZkiG8l33qj0stedbCIqGgTPg0DrIz6r5s9JqHTyZzyMfbmT3sbwLjj2eV0RGdiEXxYfRuX0QfTuGMXdDBv9MTcffx8GdwxMrxt46pBNX9I7hT1/uZEdmLvM2ZpAQEUC/jmEYY5jcL55Ve7I4Uu3hP1fN3XgYHy/DFb1jK44ZY7h3dBLbM3NZknacr7Ye4b21BxnRtT1Hcor45Hv7H5OWpB0jwMerYovnmaOScFoWgb5e3Da0EwAh/j5c2iOaeRszKKvWO7Fk5zHiwvzpGlW/f2KLDvXn3jFJBPnpWQAREWlcdWqssyxrvmVZKZZldbUs63flx35tWdanlcY8blnWw9U+t9yyrD6WZfUr//1q45bfxKrvumdZ9kxy0iXQ7TLAgvRFHitPmt78TRlM+msq73x3gNnfpl9w/JbDOQD07hAKwOS+8Ww6lM2H6w5x08AEIoJ8K8YaY3j6uj6E+HvzP++sY9mu40zuG1+xPu/VfeOxLPishhnaunI6LeZtOMzo5CjCA32rvHdN/w7EhPrxl6938siHG+kZF8q/ZgymR2wI/0y1Z5hT044zLCkCP28vwN7W+SeXpfDIpB5Vznd133iO5xWxKv3sEoqlZU6W7TrO6ORIrTksIiLNTvN6+qS5i+sHB1fDyb3266Pb4PRR6HIJdBgA/uGwa+F5TyGtg9Np8ehHm7j/rXUkRQUzJiWKBZszKtoQanNmZYve8WEAXFXecuG0LGaOPnclifbBfvz++j7sPJJHmdNicr/4ive6RQfTMy6UuRvPbbk4lV/M7CW7+ds3afztmzReX7aH0rJza1t/4CSHswurtFqc4evtYMbILqzff4qcglKem9oPP28vZo1JYueRPP6zch97jp8+Z3vm/xmfzB2VZsQBLu0RTaCvV5VaNx7KJqewVNs7i4hIs6SQ7IoRD4DxgiV/tF/vKe9HThoLDi/79+5v7BlmadW+25vFW6v2c9eIRN7/wXDuGtGZnMJSlu46dt7PbTmcTef2gYQF+AAQHx7AFb1jmDoogc7tg2r8zITesdw+rBNDEiPO2Qr5hgEdWL//FJ9vPtsGZFkWD/73e56ev50/fbWTP321k8fnbq2xJ3juhgz8vB1c1jOmxmvfOrQTydHBPHpVT3rGlc9+94snLsyfJz/bBsCYOvQTB/h6cVWfOD5Ye4gdmbkApO48jjEwqps25hARkeZHIdkVofEwaAZ8/zZkpUP6YohIgvDyxT+6jYfcDHuGWVqUMqdFeh16is9ITTuGt8Pwswkp+Hg5GNUtirAAH+ZuOH/rw6ZD2VxUPot8xst3DOKZG/qe93NPXduHd38w/Jy2hDuHJ3JRh1Ae/WgTx/OKAJiz+gCLdxzjN5N7set3k0j73SS6RQfzcqWH8M5853kbM7i0RzQh/j41XjfU34cvfzKG6SMSK475eDm4e2QXikudxLvQT/zwpB6EBnjz03e/p7jUSWraMfp2CKNdkO+FPywiItLEFJJdNeon4OUDi34Pe5fZs8dndC1fP1mrXLQ4r6SmM/65b9lyuPad4SpbsvM4Azq1qwiXvt4OJvaO5cstmRSW1LxjXnZ+CQeyCir6kRuDr7eD56b2J7eolEc+3MT+E/k8NW8rI7u1Z/rwRLy9HPh4OZg1OoltGTkVq0kAfLDuIMfziqq0cNSkpn7haUMSCA/0YVyP6Dr3E7cP9uPp6/qw5XAOT8/fxvoDp9RqISIizZZCsqtCYmHwTNj0LhTn2v3IZ4R1gKgesOtrz9UnLisqLeOVpXuwLJi95MIP353IK2Lz4WxGJ1dtE5jcL57TxWUs3lHzFuVnAnj1meSGSokJ4ecTuvPV1iNMfXkFDmN49sZ+OCptAnLNxfFEh/hVfL+DJ/N5cu5WhnSJYGKlVS3qKsTfhy8eHMOjLi69NqF3LDcM6Mjry/dSVr50nIiISHOkkFwfI38MPoGAgS5jqr6XNA72rQBnzbOJ0vx8sv4wx3KL6J8QzryNGRw8mX/e8ct2n8CyYHS1baOHJUXQPsi31paLzYfPPLTXeDPJZ9w9qgtDukSQmVPI41N6Ex8eUOV9P28v7hqZSGracTYfyubn723EaVn86aaqYdoVMaH+BPq6vvTab6b0Ij7MnyBfLy7u1K5e1xYREXE3heT6CI6GS38F/W+DwKo7iBHZDcqKIK/m2URpXpxOi9mp6fSMC+XF2wZggNeW7j3vZ1J3HiMswIc+HarOCHt7ObiyTxzfbD/C6aLScz63+VAO8WH+tA/2a8RvYPNyGP5x2wD+cdsArh/QocYxtw3tTJCvFzPfWMOK9BM8dnUvEiICG72WCwn19+GNu4fw8h2D8PXWfwWJiEjzpP+Fqq/hP4JrXzz3eKi9XS85te3cLc3Joh1H2XU0j1ljutAhPIDJ/eKZs3o/2fklNY4/szbwqG6ReNUwAzu5XzyFJU7mb6o6m1zmtPj+wCl6d2jcVovK2gf7MalPXK09wmEBPkwb0onMnEIu7RHNzYMTahzXFJJjQhilVgsREWnGFJIbW1j5LF72Qc/WIXXy8pJ04sP8ubqv/fDavaOTyC8u4z+r9tU4Pu1oHpk5hbX20g7q3I6LOoTyzILtFatNALy2dA/7s/IrtqL2lB+O7cpdIxJ55oY+2sBDRETkPBSSG1toeUjWTHKDfbrhcMWauu7w/YFTfLcni7tHdcHHy/6PQq/4UEYnR/La0j08MXcrT8zdyh+/2MGxXDvwLtlpr4NcvR/5DIfD8Keb+pNbWMqjH23CsiwO5jp59osdXN4rhikXWEnC3SKD/Xh8Sm+iQ/w9WoeIiEhzp5Dc2ALa2Q/1ZSskN8TxvCIenLOevy/e5bZrzF6ymxB/b6YN6VTl+I/HJ2MB7605wHtrDvCPb3cz6a+pLNl5jNS043SNCqJDtQfjKuseG8JDV6TwxZYjvLvmAP/cVESIvze/v16ztyIiIi2F64+my/kZY88m56jdoiEWbMrAaeG2meR9J07z+eZMZo3pSrBf1f8YDEqMYN1jl1e83pGZywNvr+PO177Dy2G4Y1jnC57/nlFJfL31KL/8YBMAL93eh0g3PLAnIiIi7qGZZHcI66CZ5AY6s4xa+rHTlJY5G/38ry7dg7fDwYyRiRcc2z02hE8fGMWtQztR5rSY0LvmLZwr83IY/nhTP0L8vRndwZuJF7m+FrGIiIh4jkKyO4R2VE9yA2RkF7B6XxZdo4IoLnOy98T51y12VdbpYt5dc4BrL44nJrRuvbkBvl48fV0fNvxmAiO61m1Vhk7tA1n+8KXcfZG2XRYREWlpFJLdIawD5GZCWc3LiMn5fbYxA8uCBy9LAWDnkcZtuXhzxT4KS5zMGpPk8mfDAnxcGh/i76M+ZBERkRZIIdkdQjsAFuTWvPOanN/cjRn0jg/l8l4xOEzj9iUXlpTx7xV7Gd8jmm7RIY12XhEREWldFJLdoWKtZLVcnHH4VAE3vbScLeVbM9fmQFY+Gw6cYnK/ePx9vEhsH1TnmeTXlu7hgbfXcTS3sNYxb6/az4nTxfWaRRYREZG2QyHZHbTrXhVOp8XP39/A6r0n+c/K/ecdO3fjYQCu6mNvupESE8KOOoRky7J4JTWdeRszuPKvqXxbvp5xZftOnOaPX+5gdHIkQ7pE1HAWEREREZtCsjto170q/r1iL8t2nSAm1I8FmzMoOc9qFXM3ZHBxp3ASIgIBSIkJZu/x0xSWlJ33GruPneZwdiF3j+xCRJAv01/7jqfnb6O41L5WmdPiZ+9uwMth+L8b+6pPWERERM5LIdkd/ELAL0wzycDuY3k88/l2xnaP4qlr+3Aqv4Rlu47XOPZYbhHbMnK4ovfZ5dJSYkNwWvZ5zic1zZ45njEykU8fGMVtQzsxe0k6N720nH0nTvPP1HTW7DvJb6f0Ji6s9o1ARERERECbibiP1kqmtMzJz97dgL+PF/93Q1/CAn0I8fdm7oYMxnaPPmf82n0nARic2K7iWPcY++G6tCN59I4PA2BbRg7+Pl50iQyqGJeadpwukUEVM9C/u64Po7pF8ssPNnLV80spLnUysXcs113cwW3fV0RERFoPzSS7i3bdY+H2o3x/4BSPT+5NdKg/ft5eTOwdy5dbMmtsn1i7LwtfLwcXdQirOJYYGYSPl6noSy4sKeP2V1Yx843VOJ0WAEWlZazYfYLRyVXXL57UJ44FD46hZ1wIEUG+/O66i9RmISIiInWikOwuYR3b/Ezy3I0ZRAT5cnXfuIpjk/vFk1tUWuODdWv3naRPxzD8vL0qjvl4OegaFczO8mXg3l97kBOni9l97DQLtx8FYN2+UxSUlDE6Oeqcc3YID+Dd+4az5BfjaK9toUVERKSOFJLdJawD5B+HktqXI2vN8otL+XrrESZdFIu319m/ZiO6ticiyJd5G6uuIV1YUsbmQzkM6tyu+qkqVrgoc9orWPTpEEaH8ABmL0kHYEnaMbwdhmFJNa9YYYzB11t/1UVERKTulBzcpY0vA7dw+1EKSsqY3C++ynFvLweTLorl661HyC8urTi++VA2xWVOBtQQkrvHhnDwZAEfrz/E3hP5/OCSrtw9qgvf7c1i/f6TpKYdY0DndoT4u7YbnoiIiEhtFJLd5cwycG0gJG8+lM3lz33LrqNnV6CYu+EwMaF+DE48d3Z3cr94CkrK+HLLkYpjZx7aG1hDSE6ODgbgd/O30SkikIkXxXLz4ARC/L35w+fb2XwohzHV+pFFREREGkIh2V1C286ue0t3HSftaB4/e/d7Ssuc5BSWsGjHMa7sE4eX49wH5YYkRtAlMojXlu3BsuyH79bsO0li+0Aia+gb7h5rr3CRdbqYmaO74OUwBPt5c/uwzqxMzwKosR9ZREREpL4Ukt3lTEhuAytc7MzMxcfLsOFgNn9fvJuvthyhuNR5TqvFGQ6HYeboLmw8mM3K9Cwsy2LdvpMM7FxzT3FCu0D8fRy0C/ThpoEJFcdnjEjE18tBeKBPlRUxRERERBpK6yS7i48/BEa2iZnkHUdyGdE1ktAAH57/Jo2kqCA6hAdwcUJ4rZ+5YUBHnvtyJ7OX7CY2zJ8Tp4trbLUAO1TPGp1Ep/ZBBPieXfkiOtSfn1/RHWOoccZaREREpL4Ukt0prEOr70kuc1qkHc1jZLdI7h/blVXpJ9h5JI/7Lkk675rE/j5eTB+RyHNf7SQlZj8AgxJrDskAP53Qvcbj945JatgXEBEREamB2i3cKbT1r5W878RpikudpMSEEB7oyx9v6kd4oA83Duh4wc/eMawzAT5ezE5NJ9Tfm25RwU1QsYiIiMiFKSS7U1jr33VvZ/lOeCkxdsAdkxLF+scuJ7l8O+nzaRfky9RBHbEsGNC5HQ61TIiIiEgzoZDsTmEdoTAbCk55uhK32ZGZhzHQLfrsLLArWz/PHJ2Ej5dheFJ7d5QnIiIiUi/qSXan6N727yObIXGUZ2txk51HcukUEUigb/3+KiVEBLLwZ2OJCfVv5MpERERE6k8zye4U19f+nbHRs3W40c4juaTUobXifBIiArVttIiIiDQrSibuFBwNwbGQ2TpDclFpGXuOn6Z7A0OyiIiISHOjkOxucX1b7UzynuOnKXVapMQqJIuIiEjropDsbrF94Nh2KCn0dCWNbkemvbKFZpJFRESktVFIdrfYvmCVwdGtnq6k0e08kou3w9AlMsjTpYiIiIg0KoVkdzvz8F4r7EvekZlHl8ggPXQnIiIirY7SjbuFJ4JfaIvtSy4oLmNbRk6N7+08kqt+ZBEREWmVFJLdzeGw+5IzN3m6knr5y9c7mfTXVB77eDOFJWUVx/OLS9mfla9+ZBEREWmVtJlIU4jtC+veAGcZOLw8XY1LFm4/SrtAH95cuY/Ve7OY1qWM43lFbM84sx21QrKIiIi0PgrJTSGuL5Tkw4ndEJXi6WrqLCO7gLSjefy/K3uQHBPCQ+9u4PEVxTy+4uuKMT3UbiEiIiKtkEJyU4jtY//O3NiiQnJq2nEARidH0TMulAUPjubFj1LplpwMQGSwH4la2UJERERaIYXkphDVA7x8IWMD9LnR09XUWWracaJC/Cpmi6ND/BnXyYexwxM9W5iIiIiIm+nBvabg5QPRPVvUMnBOp8XStGOMTo7EGOPpckRERESalEJyU4kt357asjxdSZ1sPpzNyfwSxiRHeboUERERkSankNxU4vpBQRZkpXu6kjo50488KjnSw5WIiIiIND2F5KaSMtH+veVDz9ZRR0t2HqN3fCiRwX6eLkVERESkySkkN5XwBEgYBpubf0jOKypl3f6TjFarhYiIiLRRCslNqc+NcHQrHNnq6UrOa+XuE5SUWYxRq4WIiIi0UQrJTanXtWAcsPl9T1dyXot2HCXAx4uBie08XYqIiIiIRygkN6XgKOhyCWz+oNmuclFa5uTzzZlc2jMaP++WtYW2iIiISGNRSG5qfW6Ek3vh0DpPV1KjFeknOHG6mMl94z1dioiIiIjH1CkkG2MmGmN2GGN2GWMeruH9Pxtjvi//2WmMOVXpvenGmLTyn+mNWXyL1ONqe/e9zR94upIazduQQbCfN2O766E9ERERabsuGJKNMV7Ai8AkoBdwizGmV+UxlmX9xLKs/pZl9Qf+BnxY/tkI4DfAUGAI8BtjTNtudA0Ih26X20vBOcs8XU0VxaVOFmzOYEKvGPx91GohIiIibVddZpKHALssy0q3LKsYmANcc57xtwDvlP/5CuAry7KyLMs6CXwFTGxIwa3CRddDbgYc+M7TlVSRmnaMnMJSJvdTq4WIiIi0bXUJyR2AA5VeHyw/dg5jTGegC7DQ1c+2KcmXg8Mbdi7wdCVVzN1wmPBAH0Z209JvIiIi0rZ5N/L5pgHvW5blUh+BMWYWMAsgJiaGxYsXN3JZtcvLy2vS653RL7QXvus/ZLXPpU1+7ZoUl1l8vimfIXHeLF+6pNZxnrpfLZnumWt0v1yne+Ya3S/X6Z65RvfLdc3xntUlJB8CEiq97lh+rCbTgB9V++zYap9dXP1DlmXNBmYDDBo0yBo7dmz1IW6zePFimvJ6Ffxvhc8fZmzfzhDRpemvX82CTRkUlq3jvokDzzuT7LH71YLpnrlG98t1umeu0f1yne6Za3S/XNcc71ld2i1WA8nGmC7GGF/sIPxp9UHGmB5AO2BFpcNfABOMMe3KH9ibUH5MUspbs3d+7tk6yr2/9iCRwX4M7RLh6VJEREREPO6CIdmyrFLgAexwuw1417KsLcaYJ4wxUyoNnQbMsayzu2RYlpUFPIkdtFcDT5Qfk4guENUDdni+LzntSC7fbD/K7cM64e2lpbNFRERE6tSTbFnWfGB+tWO/rvb68Vo++xrwWj3ra91SJsKKF6AwG/zDPFbGP1PT8fN2cOfwRI/VICIiItKcaNrQk7pPAmcp7PrGYyUczSnk4/WHuWlQRyKCfD1Wh4iIiEhzopDsSR0HQ0CER/uSX1++lxKnk5mjkjxWg4iIiEhzo5DsSQ4vSJ4AaV9CWWmTXz6vqJT/rNzHxN6xJEYGNfn1RURERJorhWRP6z4RCk7Cwabffe+N5XvJKSxl1hjNIouIiIhUppDsaV3Hg8OnSVsuCkvKeOzjzTz7xQ7GdY/i4k7tmuzaIiIiIi2BQrKn+YdC4kjY0TQhedfRPK59cRlvrtzHzFFdePmOQU1yXREREZGWRCG5OUiZBMd3QFa6Wy/jdFrc9+YajuYW8a8Zg/nV1b3w9dZfAREREZHqlJCag+7lu++5eTb5m+1H2X3sNL+Z3Itx3aPdei0RERGRlkwhuTlolwhRPWGne3ffm71kNx3CA7iqT5xbryMiIiLS0ikkNxcpV8C+5fbue26wbv9JVu89yT2jumjraREREZELUFq6gBcX7eL5b9Lcf6GK3fe+dsvpZ3+bTliADzcPTnDL+UVERERaE4Xk8yhzWsxeks4/Fu8mv9jNm310HAyB7d3Sl7zn+Gm+2JrJ7cM6EeTn3ejnFxEREWltlJjOY9OhbLILSgD4ettRpvSLd9/Fzuy+t/NzKCulBAd/X7SbO4Z3JiLI1+XT/Xf1ftbuOwnAjsxcfBwOpo9IbOSiRURERFonzSSfx5KdxzAGIoJ8mbvhsPsvmFK++96eb1m66zh//non8za6ft3sghIe+3gLn2/OJDXtOEdzi/jB2K5Eh/i7oWgRERGR1kczyeeRmnaMPh3CGNQ5gv+s3Ed2QQlhAT7uu2DKFRASD4ufITX6LwDsPJLr8mm+3JJJcZmT/943TLvpiYiIiNSDZpJrkVtYwrr9pxidHMnkfnEUlzn5ausR917UJwDG/hIOfkfJts8A2JmZ5/Jp5m3MoGO7APonhDd2hSIiIiJtgkJyLVbsPkGZ02J0chT9E8Lp2C6gaVou+t9OaXgSt53+NwHesONILpZl1fnjWaeLWbrrOJP7xWOMcWOhIiIiIq2XQnItUtOOE+TrxYBO7TDGcHXfeJbuOk7W6WL3XtjLm++63E8PxwF+22Ur2QUlHM0tqvPHF2zOoMxpMbmvGx8yFBEREWnlFJJrsSTtGMO7tsfX275Fk/vFUea0+Hxzptuv/U7exWynC1NOvoEPpS71Jc/dcJikqCB6xoW4sUIRERGR1k0huQb7Tpxm34l8RidHVRzrFRdKUlQQH68/5NZrlzktUndnsaTT/fjnHeAGryXsyKw5JOcVlTL15RU8OW8rRaVlHMkpZNWeLCb3VauFiIiISEMoJNcgNe04AKOTIyuOGWO4eVAC3+3NYvMh92wdDbD5UDan8kuIufhKiO3LLJ/P2ZmZU+PY3322ldV7s3h16R6ue3E5/1i8G8uyZ71FREREpP4UkmuQmnaMDuEBdIkMqnL8lqGdCPbz5uUl6Q06f0Z2AU/P30ZOYUmN1wYYmRwFw39EEgcJPPjtOeMWbT/KO98dYNaYJF6dPoiM7AJeX76XnnGhdItWq4WIiIhIQygk12DX0Tz6dgw7p2Uh1N+HW4d2Yv6mDA5k5df7/H/+aiezl6Tz20+3nvPekrTj9I4PJTLYD3pfT65PJJedeh+n8+wKFydPF/PLDzbSPSaEn16ewvieMSz48Riu6R/Pj8d3q3ddIiIiImJTSK5BdkEJ4YE1bwU9Y2QiBnh16Z56nftoTiEfrz9MVIgfH6w7yBdbzj4ImJ1fwrp9JxmTUt4L7e3Lni63Msps5Mju7yvGPfbJZk7mF/Pczf3w8/YCIDbMn79Ou5iJF6nVQkRERKShFJKrsSyrPCTXvLNeXFgAU/rH8+6aA5zKd305uH8t30uJ08k79w6ld3wo/+/DTRzPK2LL4Wyu+8cyyiyLK3rHVox3DpxBgeWLc/mLAKzff5J5GzN4YFwyvePD6vclRUREROS8FJKryS8uo6TMOu/207PGJJFfXMZbq/a7dO68olLeWrmPib1j6RYdwnNT+5NbWModr37HdS8uJ6+wlLfuGVplp7yunRP4sGw0MXs/gbxj/DM1nRB/b+4Z3aXe31FEREREzk8huZrsAvthuvDzhOQesaGM7R7F89+k8daqfXXeEe+/qw+QU1jKrDFJAHSPDeGhK1LYlpHDqORIFvx4NCO6RVb5TIi/D58FXoO3VUzWqrf4fHMmtw/rTLCfdz2/oYiIiIhciJJWNafy7ZB8vplkgGdv7MdP3/2eRz/azLJdx/n99X3P+5mSMievLd3DkMQILu7UruL4vaOTGJMSRfeYkFrXNvaL68n+Ax0p/H4B3o7uzBiR6PoXExEREZE600xyNacK7D7jsFp6ks+ICvHjjRlDeGRSD77ccoSbX15x3hnl+ZsyOHSqoGIW+QxjDD1iQ8+7+UdKbAiLSi4iIWcdN/SLJDrU34VvJCIiIiKuUkiuJqei3aLm1S0qczgM913SlV9P7sX2zFx2Hc2rcZxlWbz8bTotJOiAAAAgAElEQVRdo4K4tEe0yzV1jwnh27KLCDDF/KjrCZc/LyIiIiKuUUiupqLd4gIzyZWN624H3yXlO/VVt2zXCbZm5DBrTBIOh+vbRafEhLDS2YtSvOmYtcLlz4uIiIiIaxSSq6nLg3vVJUQEkhQZVLFbXnUvL9lNVIgf117coV41dY8NYcrgZIrjBsHuhfU6h4iIiIjUnUJyNacKSvB2GAJ9vVz63OjkSFamn6CotKzK8a2Hc0hNO85dIxIrNv5wlY+Xg2du6Etgz8shcyPk1RzGRURERKRxKCRXc2YjkfM9SFeTMSlRFJY4Wbv3ZJXj/0xNJ9DXi9uHdm54cV0vtX+nL274uURERESkVgrJ1WTnlxDqQqvFGcOS2uPjZar0JR8+VcDcDYeZNriTSz3OtYrrBwERarkQERERcTOF5GqyC0pc6kc+I8jPmwGd2lXpS/7D59sBuHtUYuMU5/CCpLF2SK7jBiYiIiIi4jqF5GpOFRRfcCOR2oxJiWLL4RyO5xXx2cYMPvn+MP87PpmO7QIbr8Cul0JeJhzb3njnFBEREZEqFJKrsXuSL7xGck1GJ9tbSn+07hC/+ngT/TqGcf/Yro1ZHnQdZ//e/lnjnldEREREKigkV3Mqv6TeM8m948NoF+jD0wu2kV9cxp+m9sfbq5FvcVhH6HIJrH4FSosb99wiIiIiAigkV1HmtMgtLK13SPZyGEZ2i8Sy4JcTe9AtOriRKyw34n8hNwM2f+Ce84uIiIi0cd6eLqA5qdiSugErUdw3pitJkUHcNSKxkaqqQbfxEN0Llv8N+k0DF5erExEREZHz00xyJafKQ3J9Z5IB+nQM46cTutdr++k6MwaGPwBHt2g5OBERERE3UEiuJLsRZpKbTJ+bICTOnk0WERERkUalkFzJqXz7QbiGzCQ3GW9fGHofpC+CzE2erkZERESkVVFIriS7ot2ifkvANbmBM8AnCFa95OlKRERERFoVheRKshuhJ7lJBYTDRdfD5g+hMMfT1YiIiIi0GgrJlWTnt7CQDDBgOpTkw5YPPV2JiIiISKuhkFzJqYISAn298PVuQbel4yB7Obi1b3i6EhEREZFWowWlQffLLighvCXNIoO9HNyAO+HwOj3AJyIiItJIFJIrOZVfQlhgC3lor7K+N4OXL6x709OViIiIiLQKCsmV5BSUEBbQAjchDIyAnlNg4xwoKfB0NSIiIiItnkJyJacKiglvKcu/VTfgTijMhq2feroSERERkRZPIbmSU/klLWtli8oSR0NEV3vNZMvydDUiIiIiLZpCciXZBSUtY0vqmjgcMPx++wG+fcs8XY2IiIhIi1ankGyMmWiM2WGM2WWMebiWMVONMVuNMVuMMW9XOl5mjPm+/KfZ9gIUlpRRVOoktKXOJAP0vw0C28Pyv3m6EhEREZEW7YJPqRljvIAXgcuBg8BqY8ynlmVtrTQmGXgEGGlZ1kljTHSlUxRYltW/ketudGd222uxM8kAPgEwZBYs/j0c3Q7RPTxdkYiIiEiLVJeZ5CHALsuy0i3LKgbmANdUG3Mv8KJlWScBLMs62rhlut+plrjbXk0G3wveAbBCs8kiIiIi9VWXkNwBOFDp9cHyY5WlACnGmGXGmJXGmImV3vM3xqwpP35tA+t1m4qZ5Ja6usUZQe3h4ttg47uQm+npakRERERaJGNdYCUEY8yNwETLsmaWv74DGGpZ1gOVxswDSoCpQEdgCdDHsqxTxpgOlmUdMsYkAQuB8ZZl7a52jVnALICYmJiBc+bMabQveCF5eXkEBwez7kgpz68v4vHh/iSGeTXZ9d3BvyCDoavu50DCtaR3nd6o5z5zv6TudM9co/vlOt0z1+h+uU73zDW6X67z1D0bN27cWsuyBtX0Xl12zjgEJFR63bH8WGUHgVWWZZUAe4wxO4FkYLVlWYcALMtKN8YsBi4GqoRky7JmA7MBBg0aZI0dO7YOZTWOxYsXM3bsWI6tOQDrNzJ+9HASIgKb7Ppuk/clndK+otOtz0FAu0Y77Zn7JXWne+Ya3S/X6Z65RvfLdbpnrtH9cl1zvGd1abdYDSQbY7oYY3yBaUD1VSo+BsYCGGMisdsv0o0x7YwxfpWOjwS20gydabcIa8kP7lU2+mdQnAurZnu6EhEREZEW54Ih2bKsUuAB4AtgG/CuZVlbjDFPGGOmlA/7AjhhjNkKLAJ+blnWCaAnsMYYs6H8+DOVV8VoTrILSnAYCPZtgdtS1yT2Iuh+Jaz8OxTleroaERERkRalTonQsqz5wPxqx35d6c8W8NPyn8pjlgN9Gl6m+53Zbc/hMJ4upfGMfgh2XAqrX4VRD3q6GhEREZEWQzvulcsuaMFbUtem40DoeimseAGK8z1djYiIiEiLoZBc7lRBCWGBLXz5t5qM+TmcPgbr3vB0JSIiIiIthkJyuez84tY3kwzQeQQkjoYlz0LBSU9XIyIiItIiKCSXyy4oIbw1hmSAK562A/Kipz1diYiIiEiLoJBc7v6x3bhuQPWNBFuJuL4w6G5Y/QpkbvZ0NSIiIiLNnkJyuamDExjXPdrTZbjPuEfBPxzm/xwusMuiiIiISFunkNxWBEbA+F/D/uWw+QNPVyMiIiLSrCkktyUD7oT4i2HBLyE309PViIiIiDRbCslticMLrnsZik/DR/eB0+npikRERESaJYXktiaqO0x6BtIXw/LnPV2NiIiISLOkkNwWDZgOva6BhU/CwbWerkZERESk2VFIbouMgcl/hZA4+PiH4CzzdEUiIiIizYpCclsV0A4mPAXHd2i1CxEREZFqFJLbsp5TIOYi+PYPUFbq6WpEREREmg2F5LbM4YBLfgkndsHm9z1djYiIiEizoZDc1vW4unw2+f80mywiIiJSTiG5rXM4YOzDkLUbNr3n6WpEREREmgWFZLFnk2P72EvC5Rz2dDUiIiIiHqeQLPaScFNegMIcePN6yM/ydEUiIiIiHqWQLLb4/nDL23bbxds321tXi4iIiLRRCslyVpcxcMOrcGgNvDcDnE5PVyQiIiLiEQrJUlWvKTDxD5D2Bax9zdPViIiIiHiEQrKca8i90PVS+PLXcHKvp6sRERERaXIKyXIuY2Dy82Ac8MkDarsQERGRNkchWWoWngBXPAV7U2HNq56uRkRERKRJKSRL7QZMt9suFvwCPrwPjqd5uiIRERGRJqGQLLUzBm58DYbdD9s+hRcG033738BZ5unKRERERNxKIVnOL6AdXPE7eHATDJ5JXObXsGO+p6sSERERcSuFZKmboEiY+AwF/jGw7HlPVyMiIiLiVgrJUnde3hzseA0c/A72r/R0NSIiIiJuo5AsLsmIG2+3YGg2WURERFoxhWRxidPLH4bMgh2fwbGdni5HRERExC0UksV1Q2aBtz+s+JunKxERERFxC4VkcV1QJPS/FTbMgczNnq5GREREpNEpJEv9jPkFBEbCWzdBzmFPVyMiIiLSqBSSpX5C4+C2d6EoF96aCoU5nq5IREREpNEoJEv9xfaBqW/A0a3w3nQoKfR0RSIiIiKNQiFZGqbbeJj8V9i9EN64GnIzPV2RiIiISIMpJEvDDbgDpv4bjmyB2ePg0DpPVyQiIiLSIN6eLkBaiV7XQEQSvHMLvHYFJI6CLmOg42DIPwEndtkP+HW9FFImgsPL0xWLiIiI1EohWRpPbB+4dxEsfQ7SF8PXj1d93ycIVr8CYQkw6G4YPBP8Qz1RqYiIiMh5KSRL4wqOgom/t/+cdwwyvofgGHuW2dsfdsyH72bDN7+FlX+HS38FF9+hmWURERFpVtSTLO4THAXJl0NcX/ALBi9v6DUF7ppnzzhHdIW5P4aXL4EDqz1drYiIiEgFhWTxjA4D4O7P4cZ/QcFJu4950dNQVuLpykREREQUksWDjIGLrof7l0Ofm+DbP8CrEyBrj6crExERkTZOIVk8zz8Mrn8ZbnodstLhlcvg0FpPVyUiIiJtmEKyNB+9r4OZX4NvILx+NaR95emKREREpI1SSJbmJTIZ7vka2neDt2+GzR94uiIRERFpgxSSpfkJiYEZ8yFhKHx8P2Rs8HRFIiIi0sYoJEvz5Bdib3Ud2B7+ezvkZ3m6IhEREWlDFJKl+QqOgpvfhNxMeP9ucJZ5uiIRERFpIxSSpXnrMBCu+hOkL4KPfwiF2Z6uSERERNoAhWRp/gbcCZc8DJvegxeHwc4vPF2RiIiItHLeni5ApE7GPQLJE+CTH8HbUyH+Yntb64gukDQWEkd5ukIRERFpRTSTLC1Hx4Fw37dw6WPgFwoHv4PUP8HrV8F7MyAnw9MVioiISCtRp5BsjJlojNlhjNlljHm4ljFTjTFbjTFbjDFvVzo+3RiTVv4zvbEKlzbK2w/GPATTP4UHN8H/Owxj/x9s/wxeGAyL/wAndnu6ShEREWnhLthuYYzxAl4ELgcOAquNMZ9alrW10phk4BFgpGVZJ40x0eXHI4DfAIMAC1hb/tmTjf9VpE3yCYCxv4S+N8GCh2Hx0/ZPzEXQ/zYYMgu81FUkIiIirqnLTPIQYJdlWemWZRUDc4Brqo25F3jxTPi1LOto+fErgK8sy8oqf+8rYGLjlC5SSUQS3PYu/GQLTHwGfIPgi0fgtSvgeJqnqxMREZEWpi4huQNwoNLrg+XHKksBUowxy4wxK40xE134rEjjCesIw34I93wJN74GWbvhpVGwajZYlqerExERkRbCWBcIDsaYG4GJlmXNLH99BzDUsqwHKo2ZB5QAU4GOwBKgDzAT8Lcs66nycY8BBZZl/bHaNWYBswBiYmIGzpkzp3G+XR3k5eURHBzcZNdr6Vra/fItyqL7jhdon7WWzJhx7Ey5H6eXb5PW0NLumafpfrlO98w1ul+u0z1zje6X6zx1z8aNG7fWsqxBNb1Xl2bNQ0BCpdcdy49VdhBYZVlWCbDHGLMTSC4fN7baZxdXv4BlWbOB2QCDBg2yxo4dW32I2yxevJimvF5L1yLv1+XXwpJniV38NLHeuTDtLQiJbbLLt8h75kG6X67TPXON7pfrdM9co/vluuZ4z+rSbrEaSDbGdDHG+ALTgE+rjfmY8jBsjInEbr9IB74AJhhj2hlj2gETyo+JNB2Hw364b+qbcHQbvDQa1v8HnE5PVyYiIiLN1AVDsmVZpcAD2OF2G/CuZVlbjDFPGGOmlA/7AjhhjNkKLAJ+blnWCcuysoAnsYP2auCJ8mMiTa/XFJj5FbTrbG9K8s+xsCe19l7lojw4uh32rYCSgqrvFebArm+grMTtZYuIiEjTq9PaWJZlzQfmVzv260p/toCflv9U/+xrwGsNK1OkkcT0hnu+gk3vw9ePwxtXQ2xfGDwTul0G+5bZay7vTYX8E2c/5xMI3cZDpxH2mLSvoKwIRj8E4x/z2NcRERER99ACstL2GGOvq9zjKtjwNqx+Deb+79n3g2MgZRK07wrhney1mHcvtMPztrkQHAuDZkBWOqx4AQbeBeEJtV5OREREWh6FZGm7fAPtGeRB98D+lXBgFSSOgvgBdh9zZT2ugknPQvZ+COtkv3/qALwwyJ6RvvFV165dlAdWGfiHNdrXERERkcajkCxiDHQebv+cj8MB7RLPvg5PgBH/A0uehaH3QcKQC1/LsuD7t+DzR6AoB8IS7N0Bu42Hi2+3Z61FRETE4+qyuoWI1Gbkg3b7xeePXHi1jJzD8PZU+6HB2D4w/jeQMBRO7IL5D8GfL4Ilf4TC7KapXURERGqlmWSRhvALhvG/hk/uh9evgu6TIOUKiEyxZ6jP2L8S3rnFXiVj4h9gyKyzLR2WBfuWw9LnYOGTsOk9mPmNfW4RERHxCM0kizRUv1vg0l/ZM8BfPQYvDoHXr7aDMRB9ZAm8MRkC2sEPUmHYD6r2PBsDiSPh9g/g1vfg2A6Y9xNtoy0iIuJBmkkWaSiHA8b83P45dQC2fgLL/gqvXQHxA+h1eJ29dNy0tyAw4vznSpkA4x6FRU9Bp2Ew+J6m+Q4iIiJShWaSRRpTeAKMeAB+vAEu+y2c2kdmzDi48+MLB+QzRv8Mul0Onz8Mh9a5t14RERGpkUKyiDv4BsKoB+Hnu9ne80Hw9qv7Zx0OuH62vV7zWzfaazSLiIhIk1JIFnGnyg/vuSIwAu74CIKi4c3rYfEz4Cxr3NpERESkVupJFmmuIpPh3m9g3k9h8e/tXue4/hDZzQ7PxaehOBf8wqD/rVoNQ0REpBEpJIs0Z75BcN1L0GUMbHgH0hfZW2lXt+RZuOQXMGA6ePs2fZ0iIiKtjEKySHNnDFx8m/0DUJgDBVngFwq+wZDxvb019vyHYOU/4Lb3oH1Xj5YsIiLS0qknWaSl8Q+1t8cOjLBnjROGwF2f2WssF5yEf10JR7d7ukoREZEWTSFZpDUwxl5jecZ8wILXr4SMDeeOsywoK23y8kRERFoatVuItCbRPWHGAvj3NTB7LHj5gcMLMFBWBGXF9riwThDdA6J7waC7oV1nT1YtIiLS7Cgki7Q27bvC3Z/D2jegtACcTrCc9lrN3v6ABSd229tf715k9zEPvx9G/dRu5RARERGFZJFWKawjXProhcdlH4JvfgtL/wzr3oS4fhAUBcHR0O8WiOnl/lpFRESaIfUki7RlYR3s3f3uXQiJI+0H//Yvh1UvwUujYP4v7GMiIiJtjGaSRQQ6DISp/z77Oj8LFj4Fq/8Jm9+HTsPBPxwC20Gv66DjQM/VKiIi0gQ0kywi5wqMgKufg1nfQsIwyNoDuxfCqtnwynh7hrko19NVioiIuI1mkkWkdnF94ZZKO/wV5sDCJ+G72bB9Hox5CHpda4dqERGRVkQzySJSd/6hcOWzcM9XENge5v0E/pgC79wK+1d6ujoREZFGo5AsIq5LGAz3LYFZi2HofXBoDfxrkr1KhmXZY5xO2LMEts2F08c9Wa2IiIjL1G4hIvVjDMRfbP+MfRg+eQC+fhwOfAeJo2DNa3Bi19nxUT2gz032eswO/f9zERFp3hSSRaTh/ELgptftpeO+/BXsmA8JQ+GSX0J4J9i33H7wb+GTkLkJrnsZfPw9XbWIiEitFJJFpHEYA8N+CMkToLSo6kYknYbBqJ/AihfsEJ2bAdPegaD2nqtXRETkPBSSRaRxte9a83FjYMT/QFgCfDgLXr0cbnuv9vFHt8Ha18HLB3yD7R+/8t/tumitZhERcSuFZBFpWr2vhZA4eGcavHIZ3DIHOg2tOiZzM7wxGYpPg3FAacG557n1PUiZ0DQ1i4hIm6OQLCJNr9NQmPk1vHWjHYavfg56Xwe+QQTl7YN/3w3e/vaY9l2hrBRKTkNRHhTnwdtTYdFTkHy5PUMtIiLSyPSIuYh4RvuucM/XEN8fPvkRPNMZXr+afhseAy9fuGve2VYML2/wD4OwDhDV3X4gMGODvaGJiIiIGygki4jnBLWHuz6DOz62H/orOEWJTxhMn1t7rzJAn6nQPhkW/g6cZXW/3tHt8J8bYfv8htcuIiKtmkKyiHiWlw90HQcTnoQfLmX1kL9BZPIFPuMN4x6BY9tgy0cXvoZl2es2zx4Lu76yHxw8sbtRyhcRkdZJIVlEWqZe10F0b1j8e3vJuZo4yyB9Mcy51d5Cu9MwmLkQHF7w/ozaPyciIm2eQrKItEwOB4x/zN7V78UhsOVje8a4tBh2L4LPHoI/9YB/X2Nvj335E3D7h/bScdf+w+5p/vKxC18nYwNk7XH/9xERkWZFq1uISMvVfZIdfL/8Fbw3HaJ6Qs4hKMqxV8dIngAX3WD/9g08+7keV8Kw+2Hl3yEoyu6H9guueu7cI/DVr2HjHHvXwPtXVT2HiIi0agrJItKydRsPSWNh/X9g3Rv2Oswpk+xj5wu1l/0WTu6zl5Jb+SIM/aG9S2BuJpzaB2vfgNJC6H8bfP8WpP4Rxv+6ab6TiIh4nEKyiLR8Di8YON3+qStvX7jlbTi4BlL/BIufPvuecUC3y2DiM/YqG5YTlj0PfadBVErj1y8iIs2OQrKItG0dB8Et79irXRTnQXAsBEXawfuMy5+EHfNh/s/gzk+1gYmISBugB/dERMCeMY7rByExVQMyQHAUjP+N/QDgxv96pj4REWlSCskiInUx8C7oOBg++gHM+ykUnPJ0RfZqHmtfx6/wmKcrERFpdRSSRUTqwuFlr6Qx9Aew9l/wwiDYNrf28ZYF6/4Nr02CF4fCH7vD30dA9sHGq+nAdzD3xyQcqMOGKiIi4hKFZBGRuvIPhUnPwKzFEBoP782AzE3njsvJgLdugk//B4pyITIFUibAqf3w3zugpLBx6vnuZQDandzQOOdzRfYh+O6f9v8ZEBFphRSSRURcFdcPbv8IAtrZW1xXDr3b5sHfh8HepTDpWbhvCdz8Jkz5G1z/MhxeB/Mfani4zMmArZ9AYHuC8g9CzuGGnc9Vq16yv8e+5U17XRGRJqKQLCJSH0Ht4ZoX4ehWWPikHXqXPAv/vQ0ikuCHy2DoLHtnwDN6XAVjfg7r37RbNhpi7b/sbbev/ov9Ov3bhp3PVftXnq1DRKQVUkgWEamvlAkw6B5Y8SK8eS0sfAr6TIUZC+zVMmoy9hHodjnM/4XdU1ybojxY/YrdrlFdaRGsec3eSbDH1RT7hMKeJgzJJQVweL29q+HWT+D0iaa7tohIE1FIFhFpiAlP2YE4fTFc+hhcPxt8/Gsf7/CCG/4JYR3h3Tvt7a+rKz4Nb98Mn/0MFj9z7vtbPobTx2DofeBwcCq8r339mlo4ik/D3B/DF49CflZ9v2VVh9aBswQu+SWUFcOGtxvnvCIizYhCsohIQ/gG2huM3PM1jHmobhuNBLSDaW9BYTa8Nx1Ki8++V5wP70yD/cshrj98NxtO7j37vmXZD+y1T4akcQCcbNcPcjPgeFrV65zaD69eYa+ysfLv8Nf+sPTP9kxwQ+xfYf8eeBckDIO1r+sBPhFpdRSSRUQaKqwDJAx27TMxve2H+favgM9+CjsWwIY5dkDekwrX/gOmvQ3Gy27jOGPZX+HQWhj2g4p+55Pt+tnvpS8+O27/Spg9Dk7tg1vfgx8uh87D4evH4f17GvR12b8SonpAYAQMmgEndsHe1IadU0SkmVFIFhHxlD43wvAH7Af53pkGH90H+5bBNS9Av2l2+B5+P2x6z25x2PBf+Po30Pt6GHh3xWkKA2IgvPPZkLxvOfz7GnvJupnfQPJlEN0Tbv0vjPkF7PgMjm6vX83OMjiwCjoNt1/3ugb8w2GNHuATkdbF29MFiIi0aROeskOvMeAfBkGR9u8zRj5otzN89API2g2Jo+G6l6qumgGQNNbuVT60zu5nDkuwHyAMjqo6bugPYPnfYOWL9ky2q45uhaKcsyHZJwD632qvmXxqP4R3cv2cIiLNkGaSRUQ8yRjoOBA6DLAfAKwckMGeDb7kYTi+AyK7w83/AW+/c8+TdAkUZcPrV4FfKNzx0bkBGeyl6/rfYs9K59VjO+szS791Gnb22LD7wcsHFjzs+vlERJophWQRkeZu0Ay48o9wx4cQEF7zmC6X2L99AuDOjyE8ofbzDbsfyorsJeZctX8FhMRXnTEOT7BXutjxGWyf7/o5RUSaIYVkEZHmzssHhtwLIbG1jwmKhKlvwozPITL5/OeLTIaUiXZIdmWlC8uCfSvsBwCrr+Ix/EcQ1RMW/MJedk5EpIWrU0g2xkw0xuwwxuwyxpzz72nGmLuMMceMMd+X/8ys9F5ZpeOfNmbxIiJSSa8pEJVSt7HDH4D847Dx3XPfKyuFwpxzj5/aD7mHz/YjV+blA1f/GbIPwLd/cK1uEZFm6IIh2RjjBbwITAJ6AbcYY3rVMPS/lmX1L/+p/G94BZWOT2mcskVEpEESR0FcP3uzksq9yWUl9u6Bz/c/d93lM8u8Ve5HrqzzcLj4dnsHwuUv2CtheMraN2CxwrqI1F9dZpKHALssy0q3LKsYmANc496yRETErYyxV7coyIL3Z9izxwBf/soOw2Ul8Ob1kJNhH0//1n4wLyIJomuaJyk34XfQ7TL48lF4dQIc3eb+71Kd02mH/2V/sb+HiEg91CUkdwAOVHp9sPxYdTcYYzYaY943xlR+YsTfGLPGGLPSGHNtQ4oVEZFGFNcPrv6LHYoXPgHfvwOrXoJhP4Lpn9oB+q2bYP1/4K0b7Qf07vrM3lq7NgHhcMscuP4VyEqHl0bDypeadke+Q2vttpCSfMjc1HTXFZFWxVgX+C8uY8yNwETLsmaWv74DGGpZ1gOVxrQH8izLKjLG3AfcbFnWpeXvdbAs65AxJglYCIy3LGt3tWvMAmYBxMTEDJwzZ07jfcMLyMvLIzg4uMmu19LpfrlO98w1ul+ua+g9S975Eh0OL8BpvMkO68XGvo9jObxol7WePpuexGGVkR3ak019fkWpT92v41OcTfcdfyPyxGqORo1kR/cHKPMOrHeddZW0+3USDnyCwUlat5kc6ji5yvv6O+Y63TPX6H65zlP3bNy4cWstyxpU03t1CcnDgccty7qi/PUjAJZl/b6W8V5AlmVZYTW89zowz7Ks92u73qBBg6w1a9act6bGtHjxYsaOHdtk12vpdL9cp3vmGt0v1zX4npUWwRtTIDcD7l1or5RxxrZ5sGcJXPY4+NYj4DqdsPx5+OYJaNcZps+FsI41jy0rgdTnIGUCxF9cn29iz1g/3x/ad4NjO+z1p6f+u8oQ/R1zne6Za3S/XOepe2aMqTUk16XdYjWQbIzpYozxBaYBVVapMMbEVXo5BdhWfrydMcav/M+RwEhgq+tfQURE3Mbbz26j+NGqqgEZoOfVcOX/1S8gg70z4KgH4a55kHsE5j5Yc+uFZcFnP4PFT9tbamdsrN/1MjfByb3QcwokDIX9q5q21UNEWo0LhmTLskqBB4AvsMPvu5ZlbTHGPGGMObNaxf8aY7YYYzYA/wvcVX68J7Cm/Pgi4BnLshSSRUSaG4XaqqIAABzfSURBVC9veyMSd+k8AsY/Bru+gk01/GPisr/8//buPDzK6uzj+PckYACBBAggQlhlV0FFVkFQEUTqggq41ap1q1oQtait1VrfurR2sXUDRVHBDUURUQFFqyiyGWRfDcgOIsgOSc77xz3TTCYzIRNCZkJ+n+vKNTPPPPPkzMkzk3vO3Oc+MHc0dLgOjqkGr1wMW5bF/nsWTwCXBK3OtyocuzbC9tWH334RKXcqFGUn7/0kYFLYtj+GXL8XuDfC474CTjrMNoqIyNGg440WIH80HJqdZUtkAyx4B6Y+CCdeAv2egC7fw6i+NqL8q4m2XHdRLZoAjbrZiHhGJ9u25huo0bikn41I+bNvB+zcCLVbxrslpUIr7omISOlISrayc/t2WKC8eCKMHQRvX28LlFz4tKVn1GpmS2tn77XqGLNeKFrKxJalsHWppVoA1G1ro9I/zDiyz0ukvPj8cXihd7lJYVKQLCIipaduGzhjGMx/C964EtZnQrchVjauYqWQ/drCTV9ARkf4YJgtcLJjXeHHXhSYLtO6v10mJUODDjaSHMmBPfGp43y4DuyB3Vvj3QopjzbMsw+55eT8U5AsIiKlq8dd0PNeC4zvWGiVMyqnFdwvLQOuHm+1nNfOhrED4eDeyMfcsgxmPAUNu0L14/O2N+wMmxfB3u0FHzPhNhup/nl9STyr0jP5D/Bcj7wFYERKy5YldrljTXzbUUoUJIuISOmqkAI974GW59mEwcI4Bx2uhctegk0L4OP7Cu6zcyO8egkkVYCLnsp/X0YnwFuQHer7/8KCtyH3IMwedTjPpvSt+Rp+XgcrP4l3S6Q82f0j7A4sYb/9h8L3PUooSBYRkcTXvLelZcweZRP9gvb9DK9eCnt+hCvetGWzQzXoYNUuQvKSXW42TLob0hraBMI5L1mt6LLgwJ680bx5r8W3LVK+bAlJTdpRzCA5N7dk2lJKFCSLiEjZcNb90OB0mPBb+GYEvHcrPNvN/nkPetkWDgmXUg3qnggrpv4v5aL+uokWaJ73OHS93UbHFo6PrS1ZX8L4W2Db9yXwxGKwaSH4XKjRBJZMipxGInIkBD+cuSTYsTb/fVtXwJQHIDcn+uN/mAWP1IdNYZWAvbe5AT7xAmgFySIiUjYkV4RLR9mEvA/vhiUfQJ02MGgMnHBO9MedPBDWfwt/bwMf3EnjrNegeR9L92jaC9JbwDfPRX7szxtsAZTp/4JVn8O6OTBmILx0PswbC29cHT1P+kjYkGmX5z4MOfth0bsld+z9u0ruWHL02bzEqsWktyyYbvHdG1brfPVX0R//5T/g4J6CaUI/ZcHTnam3YXKJN/lwFalOsoiISEJIawg3fwnZ+2zpaecO/Ziut0OTHvD10zBnNEkeOO9Ru885q9886S7LW24Qtjrt1AfguzeBkJJXKak22bBmM3jzavjwd1baLpKf19sqgNvX2LLfJ10GdVrH/ryD1mdClXRbLCW9Jcx7HU77VfGPF7RlKTzTFa54o/APHHJ0y82BPdtg1yaoWheq1s67b8sSq49cpVbBiXtbAwv/LHoXmnQveNwfV8LSwHIba2bYazJo9XQAdqS2KcEnUjIUJIuISNmSlhH7Y+q1gwHPQe8/Mfu/U+gYmrvcbjBM/ZONJocGyRvnW4B8xlDocruN4u74weowV6lp+3S/E754wuo8t78i/+/cssyqUGSHjDT/MNMWSCmuDZlwfHsL7tsNhk/+BNtWFczFjtXKaZCbDTOfV5BcXn38e5jxdF7aQ3oLuHVm3gfRzYuhZV+oUAl+CCuruHW5XS6aYGlMScn575/xjH0T1KibvQa8zztu1nSoUos9VYrxuj7ClG4hIiLlR7Xj2HNs2D/jlGoW4C4cb1Uvgqb+CSqlQrehtjrgCWfbqG0wQAboeR807g4Th1kN2SDvYeIdVsnj2g/hzqXQ+yHI+sJGg4vj4F4LVOq1t9snDwIczHujeMcLFZzYuPzjslcST0rGwnfhuJPhvL9Ch+ttdDgY/O7eCnu2Qu3WkJoB+7bD/p12X24ObFsJaY1g92YbKQ61ZxtkjoGTBkLrX9g+P4Xk8q/+0patL8q3QqVMQbKIiMiZw22lv7GD7Z981pewYgp0Hxa5hnNQcgXLk65SE8ZcBj+ttu2ZY+yff++HLACodpwF2MdUg6//U7w2bloIPsdGkgFS60PTMyFzLOQcLN4xwQL6Nd9A/dNsFDFzbPGPVZZl7y9z1RdKzO6t8PNaOOlS6HQjnHGHbV/2oV0GJ+3VaQWpDex6MC95xw+W/tT5FhtlDs+Tn/OS5SJ3+Y3VLQcbTQ4eY/saaHTGEXtqh0NBsoiIyLG14JcToHo9Kyk3cRhUr2/5yodStQ5c9Y4FWa8OsPzeyX+whU1OuTpvv0qpcNo1VsKuOHVm139rl8GRZIDOt1p+6MyR+ff9abVNlCpK9Y0da2HnehuZbtwd5r5cvoLFXZvho3vhkQz4Okpu+dEu+O1G8NxKy4C6J8HSj+x2cGXK2q1sXgDkVbgIjjbXa2+pOosm5J0/2Qdg5gibIFu3rT0+pXreaHNwol+jrkfuuR0GBckiIiIA1epaoFylJmxdaqsCVqxctMfWaWWT3nastVX89u+C/v+ApLB/s51utstvno29fRsybdJUcCQPrH50s7Phs0fzlgo+sBvGDoKpD8KT7eHli/KCnUiC+aUZneDUa2D7asj6b/T9jxbew+d/hX+1s7+HS4q+hPnRbkPwA9jJedta9rU0nD3bbCQ5pbp9cEwNpCsFJ+8FJ+2lt4C2F8OujXZO5WTbkvI7N0CX22yfpGQr4xgcSV79pX14rNv2yD/HYlCQLCIiEpRaH66dBP3+Bu0uj+2xDTvDpS/aBLjud1rgHC4tA04cAHNGw74d+e/z3uoee1/wcWA5z/Xa58/ddA76/AUO7IJp/2ePfX+IBTUDRkKv39tI32uDYNVnkY+7ZgZUPNbqSbf+BVRKs9Hko926OTDtYWjaE26dBS3OzUsrOJptWWalDUOtz7RqMZVS87a16GvpNyum2rcjtVva+Va1LiQfk/dtyNblULmmfRvTog8kp9hCN29eDd++Aj3utnz+oIxOeUvFZ023b1zCJ/olCFW3EBERCZXaADreULzHtuoHdy3PP7kvXJfbYP5b8HxvqFzDRjD3/mS5nQd2WcrD1eOtGkDQwX32lXfX3gWPV6eVtXfmCEiqaMfu9QerDw3Q9bfwnw42snzDtIITpH6YYVU9kivYT7vBtrLhnm2FP4+ybuYIyxEfMMImb9ZuBYvft76uWCnerTsycnNgdH/7QHR1yMqVG+YFlnAPcfypcGxtWPphoLLFebY9KclGlHeEBMnpze16SjVLuZg7GnD2YTP8tdQwsFT8kg9swl+Ha4/EMy0RGkkWEREpScfWKnym/vHt4YxhUP14q36RlGyTBk+5ynKMs76AT/+c/zGbF9oI9fHtIx/zzOE2CjjzOVsopfudefdVrGSpI+u/LTipav9OmxAYnFAFlnKRcwDe/Y3llB6Ndm22aibtr7DADmyk1OfCjyvi27Yjac3XVgP5+8/zVmvcvdUC3vBzKynJzqWlHwYqW4R8M5LaIGQkeVlekAyWd59SHS57KfKHzfqn2QfD6f+02426ldjTK2kaSRYRESlt5zwQ/b6De2yFv0ZnWAoAFJxYFa5KTTj/CaskMOC5grnQ7QbDV/+GTx6CVv3zRqnXzrLAMHQUsW4bGwGcdBeMu9aCneLav8t+V4WU4h8j3KZF9jX+2X8ses54uLmj7YPA6b/O2xYMArcsgeNOPPx2lobs/VbbuM2FRauVveg9u8zNhuWTA6tRFnJutewLma/a9dD0obSGVlt773Yr6ZbeIu++Fn1g+OqC52BQcKn4jd/ZSP5xJ0feLwFoJFlERCSR9H3EgojxN1nO5oxnrXpF5Rp5lQUiOfESuOZ92y9cUrIF5ttW5c83XvONjeo1OD3//h1vsHq5SybCuGtxudlFb//e7TD3FSuJ91hjW7q7pOzZZvnVM56Gb18t3jFysmHWKKu4UDskuKt1gvXFlqXRH7v4ffubJILsA/DmNZZG8+qllrJTmNxca3+r/pZXvCSwqE2kSXtBTXtZ/jFYjeSg1AybkLd5kd2u1Tz/46IFyEHBby4adrYUnwSlIFlERCSRVKxso7fZ++GlfvDRcKtDe+Y9h7fgQou+kNEZPn/MlgkGy0eu0xYqVS+4f6cboe+jsPh9Wix7JvqEwlB7f7LqHhNusxHZjI62QEmwTNjhyM2Bt38NOzdaQDv9yeLVh176gZW8Cy/vVyHFRmOjTd6bORLeuAo+uDPy/aUp5yC8fZ3VMe54o9UaHne99VE0a2dZYNv2YmjZD5ZPtQVq1mfaEuuhk/aCUqpajnxKqqUHBaVlAD5vMmjoSHJRBL+5aJy4qRagIFlERCTxpDeHq8bBL56EIfNgSCZ0vvnwjukc9P2LlYh7qhNMvh/Wzg5MpIqi8y3Q427qbZx66EVQgqsM7lxvdaOHfGfVPpIq2kTAw/XZo7DyE1v2+NyHrQTZgncO/bhwM0faiHyLPgXvq90q8kjynJcs/aRKLdiyuGB1iNKSm2tB7bjrbFS4zyPQ769w/t+sb6b8MfpjF71no8LNz4XW/eHgbgtyN8yLnusO1t8DR+f/gBYsA7fyU/v71mgU2/Nodpa1o+3FsT2ulCXuGLeIiEh51qhryS+yUP80uH2u5SZ/9aRtC69qEK7nfWxePJ06k++3EdxglYODe22FtWDwNO81mwx39h/zSn5VqwttLoBvx8BZf4Bjji16W5dPsSB433ZL4VjzFbS/ylYu9B7qtLEFU0667NBf7wdlTbeJkb0filx2LL0FLPvIRmqDedvzXof3h1pQd+ZweP5sCy7bx1gi8HD8uNJK/K2cBnu3AQ56/9lWsQPrk40L7INM9j7oNiR/ao73sHiC1dSuVB0a97DJdXNftkl7nW6K/rvTT7CfUGmBIHndHDsnQiuxFEWVmnDlW7E9Jg4UJIuIiJQn1erCRU/B6ddZUNuyX+H7JyWxpNVQ6lTca1/pN+oCm5fYMsY1m1mA1qgrTLrbJht2G5r/8af/Gha8bT+n/vLQ7ftxpa2At/xjqJJuqyBWSoMO11lNaOfsp9tQGH+j7RcM3AuTc9BGg1MbwulRSvzVbmWT2ratsmoX2Qfgw9/Z8xv4io3EVkmHVdPyB8nzXrfnV7UuVKtH2o6qQM9Dt+lQcnNsoZNPHrL6w637Q5MzoUkP65dQfR+xyYhzXoLZL9oS011/a5MQ18+1YLjX723fCoER5QXj7Ha0CaHRVK9vlz439lSLMkRBsoiISHlU/zT7KYLc5BS4/HXLyd25yQLlmk1h1ecw5X7bqVJqoLJG2Ahtwy426jtzpC3TXVhe9exRMOl3NkJ97v9Zvm2FYyLve+IlthjIF3+3fOtD5WvPHGkTzQaNgWOqRN6ndku73LLErq/81BZ96TY0r3Zy0zNtJNl7+53ZB2wZcp9rqQe7N3OSqwj9rrec3qCs6Vair90VFqAeavR7708wdrDljbfoC/3/WTAwDpVcES540hbvmPGMBcvfvQEn9La2J1W0ahVBrfuHBMkxVpiokAJVj7PV9dKbH3r/MkpBsoiIiBxatePg11Pzb+t1n5Vkm/ea5ZmGLpkd5JyNJn8wzL6eb9Ah8vEXTYCJwyxV48KnbcS7MMkVbKR00l2w4hNofk70fXduhGl/sYCx1fnR90tvAbi8vOSF79godtOeefs07WWjxpsXW7m8JRNh9xa4cpwtE541neSX+ll94ZMvy3vcZ49Yqsei9yxFofNvrCZ1tOoOX/3blne+6Blb/bGokzbTMiz3vMddMPsFq46yZ6st8hFa+eSEc2x0OrVB5El7Rfk9uzYWrGxxFNHEPRERESm+um3g3D9Ds17R9zl5oNXE/erJyFUy1nwD79xgAfTAVw4dIAed+ksb0f743sIrXUy+H3L2w3mPFR5sHlPFcnm3LLGV95ZMsqW6Q0ezm/a0y1XT7HL2KHtMs7PsdsMu7EuplTdKC7DtewuQzxwOl7xg+cAfDIORPWHtnILt2LvdRr5b/8IWPClOVZMqNW1U+Y4FcPEI6PtY/vtTqkGXW23xj+IITt47itMtFCSLiIjIkZVSDbrebqOoM0fkv2/rcnhtsJUYu/z16KkQkVRIsQoPW5cVPC7A6q9h9AUw/00bda7V7NDHDFa4WDEFDuyEEwfkvz8tw0aCV06ztmd9YXnZwTSTpCQ21+kBK6ZaXWewkXacBfUnXQo3fAoDX7bV7p4/2/K5Q4P8WSNh/882Gny4KlaGdoMKTr4Dq53dbUjxjhucGBjpuEcJBckiIiJy5PW42xay+Ogeq9ELNnFw5Nk2UnrlODg2PfbjtuhjVRs+exR2bbFt6zPh5Qvhxb6WFtHnL7Y0d1HUbmnB7/y3bJJe4x4F92naC1ZPh2+eg6QKVnUjxOY6PWwC4KJ3rWxb5lgbaQ+mozhnq+TdOtPyrmeOgPE320S9A7vh66ctb7leu9j7o7Scfj1c9GzkxWuOEspJFhERkSMvKQkufs4C13HX2mS0+W9C/Q5wyfNQs0nxjuucLXryTBerRFEhxapNVKlpk/86XBfb6HTtVpaasfh9GyGOlDPcrJeN9s5+AVpfUCA9ZFfVJpDeEuaPgxpNrLLEOQ8WPE6l6tDvcZuQN/VBu13rBCvz1r0ERpGPpLSG0L6QFSCPAgqSRUREpHSkVLWUihG9LEDuNtTqJ8daZzdc7RbQ8SaY8ZSVaes2BLoPK96EtGCFC58LbQdE3qfxGeCSwedYEB7OOavfPO1h+CLJ2tGqf/TfecYdloc8/Z9WhaJx98IXeZFSoSBZRERESk9qA7h+so2WFrEEXZH0ui+weMmFUKNx8Y8TnIhWtW70xVwqpULDzrBrs9UsjuTEARYkZ30BHa7PKyEXzTkPWrm5OS9aaorEnYJkERERKV01mwDFTK+IJqVq8SehhapU3RbXaN478qp8QZe+aCPJ0SpP1GpmHwLWzYFTrjz073UO+v/DRsDTju40hrJCQbKIiIhIqBs/O/Q+RSlTd+ZwWPYxHH9q0X6vcwqQE4iCZBEREZFQxalLHEmLPvYjZZJKwImIiIiIhFGQLCIiIiISRkGyiIiIiEgYBckiIiIiImEUJIuIiIiIhFGQLCIiIiISRkGyiIiIiEgYBckiIiIiImEUJIuIiIiIhFGQLCIiIiISRkGyiIiIiEgYBckiIiIiImEUJIuIiIiIhFGQLCIiIiISRkGyiIiIiEgYBckiIiIiImEUJIuIiIiIhFGQLCIiIiISxnnv492GfJxzW4DVpfgr04Gtpfj7yjr1V+zUZ7FRf8VOfRYb9Vfs1GexUX/FLl591sh7XzvSHQkXJJc259xs732HeLejrFB/xU59Fhv1V+zUZ7FRf8VOfRYb9VfsErHPlG4hIiIiIhJGQbKIiIiISBgFyTAi3g0oY9RfsVOfxUb9FTv1WWzUX7FTn8VG/RW7hOuzcp+TLCIiIiISTiPJIiIiIiJhym2Q7Jzr65xb6pxb4Zy7J97tSUTOuQzn3DTn3CLn3ELn3JDA9gedc+ucc5mBn37xbmuicM5lOefmB/pldmBbTefcFOfc8sBljXi3M1E451qGnEeZzrmfnXNDdY7lcc6Ncs5tds4tCNkW8Zxy5snA+9p3zrlT49fy+InSZ391zi0J9Mt451xaYHtj59zekHPt2fi1PD6i9FfU16Bz7t7AObbUOdcnPq2Oryh99kZIf2U55zID23WORY8nEvq9rFymWzjnkoFlQG9gLTALuNx7vyiuDUswzrl6QD3v/VznXDVgDnARMBDY5b3/W1wbmICcc1lAB+/91pBtjwPbvPePBj6Q1fDeD49XGxNV4HW5DugEXIvOMQCccz2AXcDL3vsTA9sinlOBQOZ2oB/Wj//y3neKV9vjJUqfnQt86r3Pds49BhDos8bAxOB+5VGU/nqQCK9B51wb4DWgI3A8MBVo4b3PKdVGx1mkPgu7/wlgh/f+IZ1jhcYTvyKB38vK60hyR2CF936V9/4A8DpwYZzblHC89xu893MD13cCi4H68W1VmXQhMDpwfTT2xiAFnQ2s9N6X5mJCCc97/19gW9jmaOfUhdg/be+9nwGkBf45lSuR+sx7P9l7nx24OQNoUOoNS1BRzrFoLgRe997v995/D6zA/qeWK4X1mXPOYYNJr5VqoxJYIfFEQr+XldcguT7wQ8jttSj4K1Tgk/ApwDeBTbcFvgIZpfSBfDww2Tk3xzl3Y2BbXe/9hsD1jUDd+DQt4Q0m/z8VnWPRRTun9N5WNNcBH4bcbuKc+9Y597lzrnu8GpWAIr0GdY4dWndgk/d+ecg2nWMBYfFEQr+XldcgWWLgnKsKvA0M9d7/DDwDNAPaAxuAJ+LYvERzhvf+VOA84NbAV3L/4y2/qfzlOB2Cc+4Y4ALgrcAmnWNFpHMqNs653wPZwJjApg1AQ+/9KcAwYKxzrnq82pdA9BosvsvJ/4Ff51hAhHjifxLxvay8BsnrgIyQ2w0C2ySMc64idkKP8d6/A+C93+S9z/He5wIjKYdftUXjvV8XuNwMjMf6ZlPwa6LA5eb4tTBhnQfM9d5vAp1jRRDtnNJ7WyGcc78C+gNXBv4hE0gb+DFwfQ6wEmgRt0YmiEJegzrHCuGcqwAMAN4IbtM5ZiLFEyT4e1l5DZJnAc2dc00CI1iDgQlxblPCCeRVvQAs9t7/PWR7aF7QxcCC8MeWR865YwMTEnDOHQuci/XNBOCawG7XAO/Fp4UJLd/Ii86xQ4p2Tk0AfhmYGd4Zmzi0IdIByhvnXF/gd8AF3vs9IdtrByaN4pxrCjQHVsWnlYmjkNfgBGCwcy7FOdcE66+Zpd2+BHYOsMR7vza4QedY9HiCBH8vq1DavzARBGY33wZ8DCQDo7z3C+PcrETUDbgamB8sZQPcB1zunGuPfS2SBdwUn+YlnLrAeHsvoAIw1nv/kXNuFvCmc+56YDU2oUMCAh8oepP/PHpc55hxzr0G9ATSnXNrgQeAR4l8Tk3CZoOvAPZgVULKnSh9di+QAkwJvEZneO9vBnoADznnDgK5wM3e+6JOYjsqROmvnpFeg977hc65N4FFWNrKreWtsgVE7jPv/QsUnFsBOscgejyR0O9l5bIEnIiIiIhIYcpruoWIiIiISFQKkkVEREREwihIFhEREREJoyBZRERERCSMgmQRERERkTAKkkVEEohzLsc5lxnyc08JHruxc041p0VEiqBc1kkWEUlge7337ePdCBGR8k4jySIiZYBzLss597hzbr5zbqZz7oTA9sbOuU+dc9855z5xzjUMbK/rnBvvnJsX+OkaOFSyc26kc26hc26yc65y3J6UiEgCU5AsIpJYKoelWwwKuW+H9/4k4D/APwPb/g2M9t6fDIwBngxsfxL43HvfDjgVCK4q2hx4ynvfFtgOXHKEn4+ISJmkFfdERBKIc26X975qhO1ZwFne+1XOuYrARu99LefcVqCe9/5gYPsG7326c24L0MB7vz/kGI2BKd775oHbw4GK3vuHj/wzExEpWzSSLCJSdvgo12OxP+R6DpqbIiISkYJkEZGyY1DI5deB618BgwPXrwS+CFz/BLgFwDmX7JxLLa1GiogcDTSCICKSWCo75zJDbn/kvQ+WgavhnPsOGw2+PLDtduBF59zdwBbg2sD2IcAI59z12IjxLcCGI956EZGjhHKSRUTKgEBOcgfv/dZ4t0VEpDxQuoWIiIiISBiNJIuIiIiIhNFIsoiIiIhIGAXJIiIiIiJhFCSLiIiIiIRRkCwiIiIiEkZBsoiIiIhIGAXJIiIiIiJh/h/oU4b/rNkctgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(xs,y_acc, ms=5, label='train accuracy')\n",
        "plt.plot(xs, y_vloss, ms=5, label='validation loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFPo1Rai9kOq"
      },
      "source": [
        "### 다른 모델로 작업"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1CLNf8GPF7q",
        "outputId": "98f74d9e-1884-4585-e2d3-3aabf5ec8994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 480)               3840      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 360)               173160    \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 240)               86640     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 120)               28920     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 60)                7260      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 30)                1830      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 31        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 301,681\n",
            "Trainable params: 301,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "t_model2 = Sequential([\n",
        "    Dense(480, input_dim = 7, activation='relu'),\n",
        "    Dense(360, activation='relu'),\n",
        "    Dense(240, activation='relu'),\n",
        "    Dense(120, activation='relu'),\n",
        "    Dense(60, activation='relu'),\n",
        "    Dense(30, activation='relu'),\n",
        "    Dense(1, activation='sigmoid'),\n",
        "])\n",
        "t_model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "VXdxJzcRPdZb"
      },
      "outputs": [],
      "source": [
        "t_model_path2 = \"t_model/best_titanic.h5\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "ck = ModelCheckpoint(\n",
        "    t_model_path2, monitor = 'val_loss', save_best_only=True, verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw-N8tkmPxdr",
        "outputId": "54dc8dc8-1303-427b-ac1b-366faa43b648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.65047, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.65047\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.65047 to 0.64935, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.64935\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.64935\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.64935\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.64935\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.64935 to 0.63026, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.63026\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.63026\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.63026\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.63026 to 0.62173, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.62173 to 0.61917, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.61917 to 0.61517, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.61517 to 0.60482, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.60482\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.60482 to 0.60069, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.60069\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.60069 to 0.58538, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.58538\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.58538\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.58538\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.58538 to 0.56831, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.56831\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.56831 to 0.55595, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.55595\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.55595 to 0.55333, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.55333\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.55333 to 0.54795, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.54795\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.54795\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.54795\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.54795 to 0.53798, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.53798\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.53798\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.53798 to 0.53425, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.53425\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.53425\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.53425\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.53425\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.53425\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.53425 to 0.52265, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.52265\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.52265 to 0.51336, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.51336\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.51336 to 0.50415, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.50415\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.50415\n"
          ]
        }
      ],
      "source": [
        "t_model2.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = \"accuracy\")\n",
        "hist2 = t_model2.fit(X_train, y_train, validation_split=0.2, verbose=0,\n",
        "                 epochs=200, batch_size=200,\n",
        "                 callbacks=[ck])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZpmc3gSQmpE",
        "outputId": "43d5ff66-20aa-4c94-e109-eef736b3c135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 4ms/step - loss: 0.4631 - accuracy: 0.8101\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.463122695684433, 0.8100558519363403]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model2 = load_model(t_model_path2)\n",
        "best_model2.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6e2GBxkQtIH"
      },
      "source": [
        "### 다른 모델로 작업2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UerB_QohQ1AN",
        "outputId": "b34fbb3a-7069-476c-bd49-999a3f2978a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_15 (Dense)            (None, 810)               6480      \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 720)               583920    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 630)               454230    \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 540)               340740    \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 450)               243450    \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 360)               162360    \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 270)               97470     \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 90)                24390     \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 91        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,913,131\n",
            "Trainable params: 1,913,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "t_model3 = Sequential([\n",
        "    Dense(810, input_dim = 7, activation='relu'),\n",
        "    Dense(720, activation='relu'),\n",
        "    Dense(630, activation='relu'),\n",
        "    Dense(540, activation='relu'),\n",
        "    Dense(450, activation='relu'),\n",
        "    Dense(360, activation='relu'),\n",
        "    Dense(270, activation='relu'),\n",
        "    Dense(90, activation='relu'),\n",
        "    Dense(1, activation='sigmoid'),\n",
        "])\n",
        "t_model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "kGLK5yrIRIVv"
      },
      "outputs": [],
      "source": [
        "t_model_path3 = \"t_model/best_titanic.h5\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "ck = ModelCheckpoint(\n",
        "    t_model_path3, monitor = 'val_loss', save_best_only=True, verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ9KuLO0RRm2",
        "outputId": "590831ff-38ed-434f-defc-a0d42754b387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.66047, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.66047\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.66047 to 0.65319, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.65319 to 0.65312, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.65312 to 0.65102, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.65102\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.65102 to 0.64439, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.64439 to 0.64258, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.64258\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.64258 to 0.63762, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.63762\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.63762\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.63762 to 0.63702, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.63702\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.63702 to 0.63691, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.63691\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.63691 to 0.61434, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.61434\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.61434 to 0.60560, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.60560\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.60560\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.60560\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.60560\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.60560 to 0.59602, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.59602 to 0.58868, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.58868\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.58868 to 0.56853, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.56853 to 0.56602, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.56602\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.56602\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.56602 to 0.55936, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.55936 to 0.55641, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.55641\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.55641\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.55641 to 0.55287, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.55287\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.55287 to 0.52883, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.52883\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.52883 to 0.52260, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.52260\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.52260\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.52260\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.52260\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.52260\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.52260\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.52260\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.52260 to 0.50119, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.50119\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.50119 to 0.49647, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.49647\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.49647\n"
          ]
        }
      ],
      "source": [
        "t_model3.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = \"accuracy\")\n",
        "hist3 = t_model3.fit(X_train, y_train, validation_split=0.2, verbose=0,\n",
        "                 epochs=500, batch_size=200,\n",
        "                 callbacks=[ck])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuXDNu0lRbmt",
        "outputId": "72d43254-c5e2-48f8-dc3a-0e6e9efadda8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 8ms/step - loss: 0.4600 - accuracy: 0.8156\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.46003803610801697, 0.8156424760818481]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model3 = load_model(t_model_path3)\n",
        "best_model3.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al1S3sdsRqKx"
      },
      "source": [
        "### 다른 모델로 작업3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7HtkP-ZR_wS",
        "outputId": "76f04c05-8a93-4135-f34b-9f82eca01e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_24 (Dense)            (None, 180)               1440      \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 160)               28960     \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 140)               22540     \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 120)               16920     \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 100)               12100     \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 80)                8080      \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 60)                4860      \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 40)                2440      \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 20)                820       \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 98,181\n",
            "Trainable params: 98,181\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "t_model4 = Sequential([\n",
        "    Dense(180, input_dim = 7, activation='relu'),\n",
        "    Dense(160, activation='relu'),\n",
        "    Dense(140, activation='relu'),\n",
        "    Dense(120, activation='relu'),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(80, activation='relu'),\n",
        "    Dense(60, activation='relu'),\n",
        "    Dense(40, activation='relu'),\n",
        "    Dense(20, activation='relu'),\n",
        "    Dense(1, activation='sigmoid'),\n",
        "])\n",
        "t_model4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "NGHqsVTgSaNJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(patience = 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "zQ9nxlMzSRwv"
      },
      "outputs": [],
      "source": [
        "t_model_path4 = \"t_model/best_titanic.h5\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "ck = ModelCheckpoint(\n",
        "    t_model_path4, monitor = 'val_loss', save_best_only=True, verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev2L_4V2Sf4M",
        "outputId": "c6bfac47-da43-4d24-c011-8b245e6466a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.64934, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.64934 to 0.64296, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.64296\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.64296 to 0.63491, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.63491\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.63491\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.63491 to 0.63473, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.63473\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.63473 to 0.62038, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.62038\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.62038\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.62038 to 0.61382, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.61382\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.61382 to 0.59957, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.59957\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.59957 to 0.59828, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.59828 to 0.58079, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.58079\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.58079 to 0.56990, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.56990\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.56990 to 0.56743, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.56743\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.56743 to 0.54655, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.54655\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.54655 to 0.53756, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.53756\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.53756\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.53756\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.53756\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.53756 to 0.53676, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.53676\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.53676 to 0.53362, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.53362\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.53362\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.53362 to 0.53093, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.53093\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.53093 to 0.52779, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.52779\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.52779\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.52779\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.52779\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.52779 to 0.52636, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.52636\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.52636 to 0.52219, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.52219 to 0.51615, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.51615\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.51615\n"
          ]
        }
      ],
      "source": [
        "t_model4.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = \"accuracy\")\n",
        "hist4 = t_model4.fit(X_train, y_train, validation_split=0.2, verbose=0,\n",
        "                 epochs=500, batch_size=200,\n",
        "                 callbacks=[ck, early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cEoU4soSoeZ",
        "outputId": "55f708b4-d56d-4ea3-faf7-798953e1d8b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 3ms/step - loss: 0.4520 - accuracy: 0.8101\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.45198971033096313, 0.8100558519363403]"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model4 = load_model(t_model_path4)\n",
        "best_model4.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPl0z-z0S2OI"
      },
      "source": [
        "### 다른 모델로 학습5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENEP4nZbSst0",
        "outputId": "5d0a4f3a-0484-42c3-a60d-9ea60175ce0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_34 (Dense)            (None, 180)               1440      \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 108)               19548     \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 54)                5886      \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 27)                1485      \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 12)                336       \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 6)                 78        \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 1)                 7         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,780\n",
            "Trainable params: 28,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "t_model5 = Sequential([\n",
        "    Dense(180, input_dim = 7, activation='relu'),\n",
        "    Dense(108, activation='relu'),\n",
        "    Dense(54, activation='relu'),\n",
        "    Dense(27, activation='relu'),\n",
        "    Dense(12, activation='relu'),\n",
        "    Dense(6, activation='relu'),\n",
        "    Dense(1, activation='sigmoid'),\n",
        "])\n",
        "t_model5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "RiBIfYNYTF25"
      },
      "outputs": [],
      "source": [
        "t_model_path5 = \"t_model/best_titanic.h5\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "ck = ModelCheckpoint(\n",
        "    t_model_path5, monitor = 'val_loss', save_best_only=True, verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVLyAm5uTLoJ",
        "outputId": "65b34c92-90f3-4aff-f70c-afeada477855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.66324, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.66324 to 0.66282, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.66282 to 0.65300, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.65300 to 0.64798, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.64798\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.64798\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.64798\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.64798\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.64798\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.64798 to 0.64647, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.64647 to 0.64261, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.64261\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.64261\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.64261 to 0.64111, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.64111 to 0.62757, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.62757 to 0.62745, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.62745\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.62745 to 0.61974, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.61974 to 0.61409, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.61409\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.61409\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.61409\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.61409\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.61409\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.61409 to 0.60617, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.60617 to 0.59786, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.59786\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.59786 to 0.59678, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.59678 to 0.59263, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.59263\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.59263\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.59263 to 0.58173, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.58173\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.58173\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.58173\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.58173\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.58173 to 0.57267, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.57267\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.57267\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.57267\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.57267\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.57267\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.57267 to 0.57101, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.57101\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.57101 to 0.56248, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.56248 to 0.56152, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.56152\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.56152 to 0.54495, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.54495\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.54495 to 0.53996, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.53996\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.53996 to 0.53964, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.53964\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.53964\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.53964\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.53964\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.53964\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.53964\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.53964\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.53964 to 0.53905, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.53905 to 0.53761, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.53761\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.53761\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.53761 to 0.52246, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.52246\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.52246 to 0.51935, saving model to t_model/best_titanic.h5\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.51935\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.51935\n"
          ]
        }
      ],
      "source": [
        "t_model5.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = \"accuracy\")\n",
        "hist5 = t_model5.fit(X_train, y_train, validation_split=0.2, verbose=0,\n",
        "                 epochs=400, batch_size=200,\n",
        "                 callbacks=[ck, early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai8oXRwFTQ3D",
        "outputId": "abae86ff-0795-42b8-a047-45620a7a56e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.8212\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.45314303040504456, 0.8212290406227112]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model5 = load_model(t_model_path5)\n",
        "best_model5.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHP-keDmTUJ1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "타이타닉-이진분류.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
